#!/usr/bin/env python3
"""
AHG-UBR5 Research Processor - GUI Main Application
AI-Powered Scientific Hypothesis Generator for UBR5 Protein Research
"""

import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext, filedialog
import threading
import os
import sys
import json
import subprocess
import queue
import time
from datetime import datetime

# Add the current directory to Python path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.scrapers.pubmed_scraper_json import main as process_pubmed
from src.scrapers.process_xrvix_dumps_json import main as process_xrvix
from src.scrapers.semantic_scholar_scraper import SemanticScholarScraper
from src.core.chromadb_manager import ChromaDBManager
from src.core.processing_config import print_config_info, get_config, DB_BATCH_SIZE

class ProgressMonitor:
    """Monitor progress from scraper processes."""
    
    def __init__(self, gui_instance):
        self.gui = gui_instance
        self.progress_queue = queue.Queue()
        self.monitoring = False
        
    def start_monitoring(self, output_widget, scraper_type):
        """Start monitoring progress for a specific scraper."""
        self.monitoring = True
        self.output_widget = output_widget
        self.scraper_type = scraper_type
        self.monitor_thread = threading.Thread(target=self._monitor_progress)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """Stop monitoring progress."""
        self.monitoring = False
        
    def _monitor_progress(self):
        """Monitor progress in a separate thread."""
        while self.monitoring:
            try:
                # Check for progress updates every 100ms
                time.sleep(0.1)
                
                # Update GUI with any new progress information
                self.gui.root.after(0, self._update_progress_display)
                
            except Exception as e:
                print(f"Progress monitoring error: {e}")
                break
                
    def _update_progress_display(self):
        """Update the progress display in the GUI."""
        try:
            # This will be called from the main thread
            # We can add specific progress parsing here if needed
            pass
        except Exception as e:
            print(f"Progress update error: {e}")

class RealTimeProgressWrapper:
    """Custom stdout wrapper to capture and forward progress to GUI in real-time."""
    
    def __init__(self, gui_instance, progress_widget, output_widget):
        self.gui = gui_instance
        self.progress_widget = progress_widget
        self.output_widget = output_widget
        self.original_stdout = sys.stdout
        self.buffer = ""
        
    def write(self, text):
        """Capture stdout and forward meaningful progress to GUI."""
        # Write to original stdout (terminal)
        self.original_stdout.write(text)
        self.original_stdout.flush()
        
        # Add to buffer
        self.buffer += text
        
        # Check for meaningful progress updates
        if '\n' in self.buffer:
            lines = self.buffer.split('\n')
            # Process complete lines
            for line in lines[:-1]:  # All but the last (incomplete) line
                self._process_line(line.strip())
            # Keep the incomplete line in buffer
            self.buffer = lines[-1]
    
    def flush(self):
        """Flush the original stdout."""
        self.original_stdout.flush()
    
    def _process_line(self, line):
        """Process a complete line and update GUI if it contains meaningful progress."""
        if not line:
            return
            
        # Look for meaningful progress patterns
        meaningful_patterns = [
            "Executing strategies:",
            "Found",
            "papers of which",
            "unique",
            "Requests per second:",
            "ETA:",
            "Starting optimized PubMed search",
            "Generated",
            "efficient search strategies",
            "Cleaned search terms:",
            "SEARCH SUMMARY",
            "Success rate:",
            "Total papers collected:",
            # UBR5/Semantic Scholar patterns
            "Searching Semantic Scholar for keyword:",
            "Added",
            "new unique papers",
            "Collected",
            "unique UBR5-related papers",
            "Processing keyword:",
            "Semantic Scholar API",
            "Rate limit:"
        ]
        
        # Check if line contains any meaningful pattern
        if any(pattern in line for pattern in meaningful_patterns):
            # Update GUI progress
            self.gui.root.after(0, lambda: self._update_gui_progress(line))
    
    def _update_gui_progress(self, line):
        """Update GUI progress from main thread."""
        try:
            # Update progress widget
            self.progress_widget.config(text=line)
            # Log to output widget
            self.gui.log_message(self.output_widget, line)
        except Exception as e:
            print(f"GUI progress update error: {e}")

class ScraperProgressWrapper:
    """Wrapper to capture progress from scrapers and send to GUI."""
    
    def __init__(self, gui_instance, progress_widget, output_widget):
        self.gui = gui_instance
        self.progress_widget = progress_widget
        self.output_widget = output_widget
        self.current_progress = "Ready to start"
        
    def update_progress(self, message):
        """Update progress message."""
        self.current_progress = message
        self.gui.root.after(0, self._update_gui)
        
    def _update_gui(self):
        """Update GUI from main thread."""
        try:
            self.progress_widget.config(text=self.current_progress)
            self.gui.log_message(self.output_widget, self.current_progress)
        except Exception as e:
            print(f"GUI update error: {e}")
            
    def run_pubmed_with_progress(self, max_results):
        """Run PubMed scraper with real-time progress monitoring."""
        try:
            self.update_progress("üîç Starting PubMed search...")
            
            # Import and run PubMed scraper with progress monitoring
            from src.scrapers.pubmed_scraper_json import search_pubmed_comprehensive
            
            # Get keywords from GUI
            keywords_str = self.gui.journal_keywords_var.get()
            if keywords_str:
                # Parse comma-separated keywords
                search_terms = [term.strip() for term in keywords_str.split(',') if term.strip()]
            else:
                # Use default terms if no keywords provided
                search_terms = ["UBR5", "ubr-5", "ubr5", "tumor immunology", "protein degradation"]
            
            # Set up real-time progress monitoring
            progress_wrapper = RealTimeProgressWrapper(
                self.gui, 
                self.progress_widget, 
                self.output_widget
            )
            
            # Redirect stdout to our wrapper (this captures output and forwards to GUI)
            old_stdout = sys.stdout
            sys.stdout = progress_wrapper
            
            try:
                # Run the scraper with real-time progress updates
                papers = search_pubmed_comprehensive(
                    search_terms=search_terms,
                    max_results=max_results
                )
                
                self.update_progress("‚úÖ PubMed search completed successfully!")
                return papers
                
            finally:
                # Restore stdout
                sys.stdout = old_stdout
                
        except Exception as e:
            self.update_progress(f"‚ùå PubMed search failed: {e}")
            return []
            
    def run_ubr5_with_progress(self, keywords):
        """Run UBR5 scraper with real-time progress monitoring."""
        try:
            self.update_progress("üîç Starting UBR5 Semantic Scholar search...")
            
            # Create UBR5 scraper
            semantic_scraper = SemanticScholarScraper()
            semantic_scraper.search_keywords = [keyword.strip() for keyword in keywords.split(',')]
            
            # Set up real-time progress monitoring
            progress_wrapper = RealTimeProgressWrapper(
                self.gui, 
                self.progress_widget, 
                self.output_widget
            )
            
            # Redirect stdout to our wrapper (this captures output and forwards to GUI)
            old_stdout = sys.stdout
            sys.stdout = progress_wrapper
            
            try:
                # Run the scraper with real-time progress updates
                semantic_scraper.run_complete_scraping()
                
                self.update_progress("‚úÖ UBR5 Semantic Scholar search completed successfully!")
                
            finally:
                # Restore stdout
                sys.stdout = old_stdout
                
        except Exception as e:
            self.update_progress(f"‚ùå UBR5 search failed: {e}")
            
    def run_xrvix_with_progress(self):
        """Run xrvix scraper with progress monitoring."""
        try:
            self.update_progress("üì• Starting preprint download and processing...")
            
            # Check if dump files exist
            dump_dir = "data/scraped_data/paperscraper_dumps"
            server_dumps_dir = os.path.join(dump_dir, "server_dumps")
            
            # Check both old and new directory structures
            biorxiv_dumps = []
            medrxiv_dumps = []
            
            if os.path.exists(server_dumps_dir):
                biorxiv_dumps = [f for f in os.listdir(server_dumps_dir) if 'biorxiv' in f.lower()]
                medrxiv_dumps = [f for f in os.listdir(server_dumps_dir) if 'medrxiv' in f.lower()]
            elif os.path.exists(dump_dir):
                biorxiv_dumps = [f for f in os.listdir(dump_dir) if f.startswith('biorxiv')]
                medrxiv_dumps = [f for f in os.listdir(dump_dir) if f.startswith('medrxiv')]
            
            if not biorxiv_dumps and not medrxiv_dumps:
                self.update_progress("üì• No preprint dumps found! Downloading...")
                
                # Download dumps
                from src.scrapers.xrvix_downloader import XRXivDownloader
                downloader = XRXivDownloader()
                
                # Monitor download progress
                self.update_progress("üì• Downloading Biorxiv dump (this may take ~1 hour)...")
                biorxiv_file = downloader.download_biorxiv_dump()
                if biorxiv_file:
                    self.update_progress("‚úÖ Biorxiv dump downloaded successfully!")
                else:
                    self.update_progress("‚ùå Failed to download Biorxiv dump")
                    
                self.update_progress("üì• Downloading Medrxiv dump (this may take ~30 minutes)...")
                medrxiv_file = downloader.download_medrxiv_dump()
                if medrxiv_file:
                    self.update_progress("‚úÖ Medrxiv dump downloaded successfully!")
                else:
                    self.update_progress("‚ùå Failed to download Medrxiv dump")
            else:
                self.update_progress(f"‚úÖ Found {len(biorxiv_dumps)} Biorxiv dumps and {len(medrxiv_dumps)} Medrxiv dumps")
            
            # Process dumps
            self.update_progress("üîÑ Processing preprint dumps...")
            process_xrvix()
            self.update_progress("‚úÖ Preprint processing completed successfully!")
            
        except Exception as e:
            self.update_progress(f"‚ùå Preprint processing failed: {e}")

class AIResearchProcessorGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("AI Research Processor")
        self.root.geometry("1200x800")
        self.root.minsize(1000, 600)
        
        # Initialize progress monitor
        self.progress_monitor = ProgressMonitor(self)
        
        # Initialize configuration variables
        self.init_config_variables()
        
        # Configure style
        self.setup_styles()
        
        # Create main notebook for tabs
        self.notebook = ttk.Notebook(root)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Create tabs
        self.create_tabs()
        
        # Status bar
        self.create_status_bar()
        
    def setup_styles(self):
        """Configure ttk styles for better appearance."""
        style = ttk.Style()
        style.theme_use('clam')
        
        # Configure notebook style
        style.configure('TNotebook.Tab', padding=[20, 10])
        
        # Configure button styles
        style.configure('Action.TButton', padding=[10, 5])
        style.configure('Danger.TButton', padding=[10, 5])
        
    def create_tabs(self):
        """Create all main tabs."""
        # Paper Scraping & Processing Tab
        self.paper_tab = ttk.Frame(self.notebook)
        self.notebook.add(self.paper_tab, text="üìö Paper Scraping & Processing")
        self.create_paper_scraping_tab()
        
        # Vector Database Management Tab
        self.vector_tab = ttk.Frame(self.notebook)
        self.notebook.add(self.vector_tab, text="üóÑÔ∏è Vector Database Management")
        self.create_vector_db_tab()
        
        # Settings & Config Tab
        self.settings_tab = ttk.Frame(self.notebook)
        self.notebook.add(self.settings_tab, text="‚öôÔ∏è Settings & Config")
        self.create_settings_tab()
        
        # Hypothesis Generation Tab
        self.hypothesis_tab = ttk.Frame(self.notebook)
        self.notebook.add(self.hypothesis_tab, text="üß† Hypothesis Generation")
        self.create_hypothesis_tab()
        
        # Tutorial Tab
        self.tutorial_tab = ttk.Frame(self.notebook)
        self.notebook.add(self.tutorial_tab, text="üìñ Tutorial")
        self.create_tutorial_tab()
        
    def create_status_bar(self):
        """Create status bar at bottom of window."""
        self.status_frame = ttk.Frame(self.root)
        self.status_frame.pack(fill=tk.X, side=tk.BOTTOM)
        
        self.status_label = ttk.Label(self.status_frame, text="Ready")
        self.status_label.pack(side=tk.LEFT, padx=5, pady=2)
        
        self.progress_bar = ttk.Progressbar(self.status_frame, mode='indeterminate')
        self.progress_bar.pack(side=tk.RIGHT, padx=5, pady=2)
        
    def update_status(self, message):
        """Update status bar message."""
        self.status_label.config(text=message)
        self.root.update_idletasks()
        
    def start_progress(self):
        """Start progress bar animation."""
        self.progress_bar.start()
        
    def stop_progress(self):
        """Stop progress bar animation."""
        self.progress_bar.stop()
        
    def log_message(self, text_widget, message):
        """Add message to text widget with timestamp."""
        import datetime
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        text_widget.insert(tk.END, f"[{timestamp}] {message}\n")
        text_widget.see(tk.END)  # Auto-scroll to bottom
        text_widget.update_idletasks()  # Ensure the widget updates immediately
        self.root.update_idletasks()
        
    def create_paper_scraping_tab(self):
        """Create Paper Scraping & Processing tab with subtabs."""
        # Create scrollable frame
        canvas = tk.Canvas(self.paper_tab)
        scrollbar = ttk.Scrollbar(self.paper_tab, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Pack canvas and scrollbar
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        # Create notebook for subtabs
        paper_notebook = ttk.Notebook(scrollable_frame)
        paper_notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Bind mousewheel to canvas
        def _on_mousewheel(event):
            canvas.yview_scroll(int(-1*(event.delta/120)), "units")
        canvas.bind_all("<MouseWheel>", _on_mousewheel)
        
        # Full Scraper subtab
        full_scraper_frame = ttk.Frame(paper_notebook)
        paper_notebook.add(full_scraper_frame, text="Full Scraper")
        self.create_full_scraper_subtab(full_scraper_frame)
        
        # Journal Articles subtab
        journal_frame = ttk.Frame(paper_notebook)
        paper_notebook.add(journal_frame, text="Journal Articles Only")
        self.create_journal_articles_subtab(journal_frame)
        
        # Preprints subtab
        preprints_frame = ttk.Frame(paper_notebook)
        paper_notebook.add(preprints_frame, text="Preprints Only")
        self.create_preprints_subtab(preprints_frame)
        
        # Generate Embeddings subtab
        embeddings_frame = ttk.Frame(paper_notebook)
        paper_notebook.add(embeddings_frame, text="Generate Embeddings")
        self.create_embeddings_subtab(embeddings_frame)
        
    def create_full_scraper_subtab(self, parent):
        """Create full scraper subtab."""
        # Title
        title_label = ttk.Label(parent, text="Full Scraper (Preprints + Journal Articles)", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """This option runs a complete scraping process including:
‚Ä¢ PubMed (Journal Articles) - Custom keywords
‚Ä¢ xrvix (Preprints) - Biorxiv, Medrxiv (no keywords needed)
‚Ä¢ UBR5 API (Semantic Scholar) - Same keywords as PubMed"""
        
        desc_label = ttk.Label(parent, text=desc_text, justify=tk.LEFT)
        desc_label.pack(pady=(0, 20))
        
        # Keywords configuration
        keywords_frame = ttk.LabelFrame(parent, text="Search Keywords Configuration", padding=10)
        keywords_frame.pack(fill=tk.X, pady=(0, 20))
        
        # Load saved keywords
        self.full_scraper_keywords_var = tk.StringVar()
        self.load_saved_keywords_unified()
        
        ttk.Label(keywords_frame, text="Keywords (used for both PubMed and Semantic Scholar):").pack(anchor=tk.W)
        keywords_entry = ttk.Entry(keywords_frame, textvariable=self.full_scraper_keywords_var, width=60)
        keywords_entry.pack(fill=tk.X, pady=(0, 10))
        
        # PubMed max results
        max_results_frame = ttk.Frame(keywords_frame)
        max_results_frame.pack(fill=tk.X, pady=(0, 10))
        
        ttk.Label(max_results_frame, text="Max PubMed Results:").pack(side=tk.LEFT)
        self.max_results_var = tk.StringVar(value="5000")
        max_results_entry = ttk.Entry(max_results_frame, textvariable=self.max_results_var, width=10)
        max_results_entry.pack(side=tk.LEFT, padx=(10, 0))
        
        # Buttons
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        run_button = ttk.Button(button_frame, text="üöÄ Run Full Scraper", 
                               command=self.run_full_scraper_gui, style='Action.TButton')
        run_button.pack(side=tk.LEFT, padx=(0, 10))
        
        save_keywords_button = ttk.Button(button_frame, text="üíæ Save Keywords", 
                                        command=self.save_keywords_unified_gui)
        save_keywords_button.pack(side=tk.LEFT)
        
        # Progress area
        progress_frame = ttk.LabelFrame(parent, text="Real-time Progress", padding=10)
        progress_frame.pack(fill=tk.X, pady=(0, 10))
        
        # Progress display for each scraper
        self.full_scraper_progress = ttk.Label(progress_frame, text="Ready to start", 
                                             font=('Courier', 9), foreground='blue')
        self.full_scraper_progress.pack(anchor=tk.W)
        
        # Output area
        output_frame = ttk.LabelFrame(parent, text="Output", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True)
        
        self.full_scraper_output = scrolledtext.ScrolledText(output_frame, height=15)
        self.full_scraper_output.pack(fill=tk.BOTH, expand=True)
        
    def create_journal_articles_subtab(self, parent):
        """Create journal articles only subtab."""
        # Title
        title_label = ttk.Label(parent, text="Journal Articles Only (PubMed + Semantic Scholar)", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """This option scrapes journal articles from:
‚Ä¢ PubMed - Custom keywords
‚Ä¢ Semantic Scholar (UBR5 API) - Custom keywords"""
        
        desc_label = ttk.Label(parent, text=desc_text, justify=tk.LEFT)
        desc_label.pack(pady=(0, 20))
        
        # Keywords configuration (reuse from full scraper)
        keywords_frame = ttk.LabelFrame(parent, text="Search Keywords Configuration", padding=10)
        keywords_frame.pack(fill=tk.X, pady=(0, 20))
        
        ttk.Label(keywords_frame, text="Keywords (used for both PubMed and Semantic Scholar):").pack(anchor=tk.W)
        self.journal_keywords_var = tk.StringVar()
        journal_entry = ttk.Entry(keywords_frame, textvariable=self.journal_keywords_var, width=60)
        journal_entry.pack(fill=tk.X, pady=(0, 10))
        
        # PubMed max results
        max_results_frame = ttk.Frame(keywords_frame)
        max_results_frame.pack(fill=tk.X, pady=(0, 10))
        
        ttk.Label(max_results_frame, text="Max PubMed Results:").pack(side=tk.LEFT)
        self.journal_max_results_var = tk.StringVar(value="5000")
        journal_max_results_entry = ttk.Entry(max_results_frame, textvariable=self.journal_max_results_var, width=10)
        journal_max_results_entry.pack(side=tk.LEFT, padx=(10, 0))
        
        # Buttons
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        run_button = ttk.Button(button_frame, text="üìö Run Journal Articles Scraper", 
                               command=self.run_journal_articles_gui, style='Action.TButton')
        run_button.pack(side=tk.LEFT)
        
        # Progress area
        progress_frame = ttk.LabelFrame(parent, text="Real-time Progress", padding=10)
        progress_frame.pack(fill=tk.X, pady=(0, 10))
        
        # Progress display for each scraper
        self.journal_progress = ttk.Label(progress_frame, text="Ready to start", 
                                        font=('Courier', 9), foreground='blue')
        self.journal_progress.pack(anchor=tk.W)
        
        # Output area
        output_frame = ttk.LabelFrame(parent, text="Output", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True)
        
        self.journal_output = scrolledtext.ScrolledText(output_frame, height=15)
        self.journal_output.pack(fill=tk.BOTH, expand=True)
        
    def create_preprints_subtab(self, parent):
        """Create preprints only subtab."""
        # Title
        title_label = ttk.Label(parent, text="Preprints Only (Biorxiv, Medrxiv)", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """This option processes preprint data from:
‚Ä¢ Biorxiv - Automatically downloads and processes
‚Ä¢ Medrxiv - Automatically downloads and processes

Note: Custom keywords are not applicable for this option.
The system will process all available preprint data."""
        
        desc_label = ttk.Label(parent, text=desc_text, justify=tk.LEFT)
        desc_label.pack(pady=(0, 20))
        
        # Status check
        status_frame = ttk.LabelFrame(parent, text="Preprint Data Status", padding=10)
        status_frame.pack(fill=tk.X, pady=(0, 20))
        
        self.preprints_status_label = ttk.Label(status_frame, text="Checking status...")
        self.preprints_status_label.pack(anchor=tk.W)
        
        # Buttons
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        check_status_button = ttk.Button(button_frame, text="üîç Check Status", 
                                       command=self.check_preprints_status)
        check_status_button.pack(side=tk.LEFT, padx=(0, 10))
        
        run_button = ttk.Button(button_frame, text="üìÑ Run Preprints Scraper", 
                               command=self.run_preprints_gui, style='Action.TButton')
        run_button.pack(side=tk.LEFT)
        
        # Progress area
        progress_frame = ttk.LabelFrame(parent, text="Real-time Progress", padding=10)
        progress_frame.pack(fill=tk.X, pady=(0, 10))
        
        # Progress display for each scraper
        self.preprints_progress = ttk.Label(progress_frame, text="Ready to start", 
                                          font=('Courier', 9), foreground='blue')
        self.preprints_progress.pack(anchor=tk.W)
        
        # Output area
        output_frame = ttk.LabelFrame(parent, text="Output", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True)
        
        self.preprints_output = scrolledtext.ScrolledText(output_frame, height=15)
        self.preprints_output.pack(fill=tk.BOTH, expand=True)
        
        # Check status on load
        self.root.after(100, self.check_preprints_status)
        
    def create_embeddings_subtab(self, parent):
        """Create generate embeddings subtab."""
        # Title
        title_label = ttk.Label(parent, text="Generate Embeddings", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """This option generates embeddings for processed data.
Embeddings are typically generated during the scraping process.
Use this option to regenerate embeddings if needed."""
        
        desc_label = ttk.Label(parent, text=desc_text, justify=tk.LEFT)
        desc_label.pack(pady=(0, 20))
        
        # Status check
        status_frame = ttk.LabelFrame(parent, text="Embeddings Status", padding=10)
        status_frame.pack(fill=tk.X, pady=(0, 20))
        
        self.embeddings_status_label = ttk.Label(status_frame, text="Checking status...")
        self.embeddings_status_label.pack(anchor=tk.W)
        
        # Buttons
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        check_status_button = ttk.Button(button_frame, text="üîç Check Status", 
                                       command=self.check_embeddings_status)
        check_status_button.pack(side=tk.LEFT, padx=(0, 10))
        
        generate_button = ttk.Button(button_frame, text="üîÑ Generate Embeddings", 
                                   command=self.generate_embeddings_gui, style='Action.TButton')
        generate_button.pack(side=tk.LEFT)
        
        # Output area
        output_frame = ttk.LabelFrame(parent, text="Output", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True)
        
        self.embeddings_output = scrolledtext.ScrolledText(output_frame, height=15)
        self.embeddings_output.pack(fill=tk.BOTH, expand=True)
        
        # Check status on load
        self.root.after(100, self.check_embeddings_status)
        
    def create_vector_db_tab(self):
        """Create Vector Database Management tab with subtabs."""
        # Create scrollable frame
        canvas = tk.Canvas(self.vector_tab)
        scrollbar = ttk.Scrollbar(self.vector_tab, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Pack canvas and scrollbar
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        # Create notebook for subtabs
        vector_notebook = ttk.Notebook(scrollable_frame)
        vector_notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Bind mousewheel to canvas
        def _on_mousewheel(event):
            canvas.yview_scroll(int(-1*(event.delta/120)), "units")
        canvas.bind_all("<MouseWheel>", _on_mousewheel)
        
        # Load Embeddings subtab
        load_frame = ttk.Frame(vector_notebook)
        vector_notebook.add(load_frame, text="Load Embeddings")
        self.create_load_embeddings_subtab(load_frame)
        
        # Show ChromaDB Data subtab
        show_frame = ttk.Frame(vector_notebook)
        vector_notebook.add(show_frame, text="Show ChromaDB Data")
        self.create_show_chromadb_subtab(show_frame)
        
        # Clear ChromaDB Data subtab
        clear_frame = ttk.Frame(vector_notebook)
        vector_notebook.add(clear_frame, text="Clear ChromaDB Data")
        self.create_clear_chromadb_subtab(clear_frame)
        
    def create_load_embeddings_subtab(self, parent):
        """Create load embeddings subtab."""
        # Title
        title_label = ttk.Label(parent, text="Load Embeddings into ChromaDB", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """This option loads processed embeddings into the ChromaDB vector database.
The system will load embeddings from:
‚Ä¢ PubMed embeddings
‚Ä¢ xrvix embeddings (biorxiv, medrxiv)
‚Ä¢ UBR5 API embeddings"""
        
        desc_label = ttk.Label(parent, text=desc_text, justify=tk.LEFT)
        desc_label.pack(pady=(0, 20))
        
        # Status check
        status_frame = ttk.LabelFrame(parent, text="ChromaDB Status", padding=10)
        status_frame.pack(fill=tk.X, pady=(0, 20))
        
        self.load_status_label = ttk.Label(status_frame, text="Checking status...")
        self.load_status_label.pack(anchor=tk.W)
        
        # Buttons
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        check_status_button = ttk.Button(button_frame, text="üîç Check Status", 
                                       command=self.check_chromadb_status)
        check_status_button.pack(side=tk.LEFT, padx=(0, 10))
        
        load_button = ttk.Button(button_frame, text="üì• Load Embeddings", 
                               command=self.load_embeddings_gui, style='Action.TButton')
        load_button.pack(side=tk.LEFT)
        
        # Output area
        output_frame = ttk.LabelFrame(parent, text="Output", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True)
        
        self.load_output = scrolledtext.ScrolledText(output_frame, height=15)
        self.load_output.pack(fill=tk.BOTH, expand=True)
        
        # Check status on load
        self.root.after(100, self.check_chromadb_status)
        
    def create_show_chromadb_subtab(self, parent):
        """Create show ChromaDB data subtab."""
        # Title
        title_label = ttk.Label(parent, text="Show Current ChromaDB Data", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """This option displays information about the current ChromaDB collections
and their contents, including document counts and source breakdown."""
        
        desc_label = ttk.Label(parent, text=desc_text, justify=tk.LEFT)
        desc_label.pack(pady=(0, 20))
        
        # Buttons
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        show_button = ttk.Button(button_frame, text="üìä Show ChromaDB Data", 
                                command=self.show_chromadb_data_gui, style='Action.TButton')
        show_button.pack(side=tk.LEFT)
        
        refresh_button = ttk.Button(button_frame, text="üîÑ Refresh", 
                                   command=self.refresh_chromadb_data)
        refresh_button.pack(side=tk.LEFT, padx=(10, 0))
        
        # Output area
        output_frame = ttk.LabelFrame(parent, text="ChromaDB Information", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True)
        
        self.show_output = scrolledtext.ScrolledText(output_frame, height=15)
        self.show_output.pack(fill=tk.BOTH, expand=True)
        
    def create_clear_chromadb_subtab(self, parent):
        """Create clear ChromaDB data subtab."""
        # Title
        title_label = ttk.Label(parent, text="Clear ChromaDB Data", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """‚ö†Ô∏è WARNING: This option will permanently delete all data from ChromaDB.
This action cannot be undone. Make sure you have backups if needed."""
        
        desc_label = ttk.Label(parent, text=desc_text, justify=tk.LEFT, foreground='red')
        desc_label.pack(pady=(0, 20))
        
        # Confirmation frame
        confirm_frame = ttk.LabelFrame(parent, text="Confirmation Required", padding=10)
        confirm_frame.pack(fill=tk.X, pady=(0, 20))
        
        self.confirm_var = tk.BooleanVar()
        confirm_check = ttk.Checkbutton(confirm_frame, 
                                       text="I understand this will delete all ChromaDB data",
                                       variable=self.confirm_var)
        confirm_check.pack(anchor=tk.W)
        
        # Buttons
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        clear_button = ttk.Button(button_frame, text="üóëÔ∏è Clear ChromaDB Data", 
                                 command=self.clear_chromadb_data_gui, 
                                 style='Danger.TButton')
        clear_button.pack(side=tk.LEFT)
        
        # Output area
        output_frame = ttk.LabelFrame(parent, text="Output", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True)
        
        self.clear_output = scrolledtext.ScrolledText(output_frame, height=15)
        self.clear_output.pack(fill=tk.BOTH, expand=True)
        
    def create_settings_tab(self):
        """Create Settings & Config tab with subtabs."""
        # Create scrollable frame
        canvas = tk.Canvas(self.settings_tab)
        scrollbar = ttk.Scrollbar(self.settings_tab, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Pack canvas and scrollbar
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        # Create notebook for subtabs
        settings_notebook = ttk.Notebook(scrollable_frame)
        settings_notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Bind mousewheel to canvas
        def _on_mousewheel(event):
            canvas.yview_scroll(int(-1*(event.delta/120)), "units")
        canvas.bind_all("<MouseWheel>", _on_mousewheel)
        
        # Data Status subtab
        status_frame = ttk.Frame(settings_notebook)
        settings_notebook.add(status_frame, text="Data Status")
        self.create_data_status_subtab(status_frame)
        
        # Configurations subtab
        config_frame = ttk.Frame(settings_notebook)
        settings_notebook.add(config_frame, text="Configurations")
        self.create_configurations_subtab(config_frame)
        
    def create_data_status_subtab(self, parent):
        """Create data status subtab."""
        # Title
        title_label = ttk.Label(parent, text="Data Status Overview", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """This option shows the status of all available data sources
and provides recommendations for next steps."""
        
        desc_label = ttk.Label(parent, text=desc_text, justify=tk.LEFT)
        desc_label.pack(pady=(0, 20))
        
        # Buttons
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        show_status_button = ttk.Button(button_frame, text="üìä Show Data Status", 
                                       command=self.show_data_status_gui, style='Action.TButton')
        show_status_button.pack(side=tk.LEFT)
        
        refresh_button = ttk.Button(button_frame, text="üîÑ Refresh", 
                                   command=self.refresh_data_status)
        refresh_button.pack(side=tk.LEFT, padx=(10, 0))
        
        # Output area
        output_frame = ttk.LabelFrame(parent, text="Data Status Information", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True)
        
        self.data_status_output = scrolledtext.ScrolledText(output_frame, height=15)
        self.data_status_output.pack(fill=tk.BOTH, expand=True)
        
    def create_configurations_subtab(self, parent):
        """Create comprehensive configuration editor with subtabs."""
        # Create notebook for configuration subtabs
        config_notebook = ttk.Notebook(parent)
        config_notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Processing Configuration Tab
        processing_frame = ttk.Frame(config_notebook)
        config_notebook.add(processing_frame, text="üîÑ Processing")
        self.create_processing_config_tab(processing_frame)
        
        # Rate Limiting Configuration Tab
        rate_frame = ttk.Frame(config_notebook)
        config_notebook.add(rate_frame, text="‚è±Ô∏è Rate Limiting")
        self.create_rate_limiting_config_tab(rate_frame)
        
        # Database Configuration Tab
        db_frame = ttk.Frame(config_notebook)
        config_notebook.add(db_frame, text="üóÑÔ∏è Database")
        self.create_database_config_tab(db_frame)
        
        # API Configuration Tab
        api_frame = ttk.Frame(config_notebook)
        config_notebook.add(api_frame, text="üîå API")
        self.create_api_config_tab(api_frame)
        
        # Search Configuration Tab
        search_frame = ttk.Frame(config_notebook)
        config_notebook.add(search_frame, text="üîç Search")
        self.create_search_config_tab(search_frame)
        
        # Save/Load buttons at the bottom
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, padx=10, pady=(0, 10))
        
        save_config_button = ttk.Button(button_frame, text="üíæ Save All Configurations", 
                                      command=self.save_all_configurations, style='Action.TButton')
        save_config_button.pack(side=tk.LEFT, padx=(0, 10))
        
        load_config_button = ttk.Button(button_frame, text="üìÇ Load Configurations", 
                                      command=self.load_all_configurations, style='Action.TButton')
        load_config_button.pack(side=tk.LEFT, padx=(0, 10))
        
        reset_config_button = ttk.Button(button_frame, text="üîÑ Reset to Defaults", 
                                      command=self.reset_configurations, style='Action.TButton')
        reset_config_button.pack(side=tk.LEFT)
    
    def create_processing_config_tab(self, parent):
        """Create processing configuration tab."""
        # Create scrollable frame
        canvas = tk.Canvas(parent)
        scrollbar = ttk.Scrollbar(parent, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Processing Settings
        processing_frame = ttk.LabelFrame(scrollable_frame, text="Parallel Processing", padding=10)
        processing_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # MAX_WORKERS
        ttk.Label(processing_frame, text="Max Workers:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.max_workers_var = tk.StringVar(value="8")
        max_workers_entry = ttk.Entry(processing_frame, textvariable=self.max_workers_var, width=10)
        max_workers_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(processing_frame, text="(Number of parallel workers for processing)").grid(row=0, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # BATCH_SIZE
        ttk.Label(processing_frame, text="Batch Size:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.batch_size_var = tk.StringVar(value="500")
        batch_size_entry = ttk.Entry(processing_frame, textvariable=self.batch_size_var, width=10)
        batch_size_entry.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(processing_frame, text="(Papers processed per batch)").grid(row=1, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # REQUEST_TIMEOUT
        ttk.Label(processing_frame, text="Request Timeout:").grid(row=2, column=0, sticky=tk.W, pady=2)
        self.request_timeout_var = tk.StringVar(value="30")
        request_timeout_entry = ttk.Entry(processing_frame, textvariable=self.request_timeout_var, width=10)
        request_timeout_entry.grid(row=2, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(processing_frame, text="(Seconds)").grid(row=2, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # SAVE_INTERVAL
        ttk.Label(processing_frame, text="Save Interval:").grid(row=3, column=0, sticky=tk.W, pady=2)
        self.save_interval_var = tk.StringVar(value="1000")
        save_interval_entry = ttk.Entry(processing_frame, textvariable=self.save_interval_var, width=10)
        save_interval_entry.grid(row=3, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(processing_frame, text="(Save progress every N papers)").grid(row=3, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Text Processing Settings
        text_frame = ttk.LabelFrame(scrollable_frame, text="Text Processing", padding=10)
        text_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # MIN_CHUNK_LENGTH
        ttk.Label(text_frame, text="Min Chunk Length:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.min_chunk_length_var = tk.StringVar(value="50")
        min_chunk_entry = ttk.Entry(text_frame, textvariable=self.min_chunk_length_var, width=10)
        min_chunk_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(text_frame, text="(Minimum characters per text chunk)").grid(row=0, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # MAX_CHUNK_LENGTH
        ttk.Label(text_frame, text="Max Chunk Length:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.max_chunk_length_var = tk.StringVar(value="8000")
        max_chunk_entry = ttk.Entry(text_frame, textvariable=self.max_chunk_length_var, width=10)
        max_chunk_entry.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(text_frame, text="(Maximum characters per text chunk)").grid(row=1, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
    
    def create_rate_limiting_config_tab(self, parent):
        """Create rate limiting configuration tab."""
        # Create scrollable frame
        canvas = tk.Canvas(parent)
        scrollbar = ttk.Scrollbar(parent, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # General Rate Limiting
        general_frame = ttk.LabelFrame(scrollable_frame, text="General Rate Limiting", padding=10)
        general_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # RATE_LIMIT_DELAY
        ttk.Label(general_frame, text="Rate Limit Delay:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.rate_limit_delay_var = tk.StringVar(value="0.02")
        rate_limit_delay_entry = ttk.Entry(general_frame, textvariable=self.rate_limit_delay_var, width=10)
        rate_limit_delay_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(general_frame, text="(Seconds between requests)").grid(row=0, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Citation Rate Limiting
        citation_frame = ttk.LabelFrame(scrollable_frame, text="Citation Processing", padding=10)
        citation_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # CITATION_RATE_LIMIT
        ttk.Label(citation_frame, text="Citation Rate Limit:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.citation_rate_limit_var = tk.StringVar(value="0.5")
        citation_rate_limit_entry = ttk.Entry(citation_frame, textvariable=self.citation_rate_limit_var, width=10)
        citation_rate_limit_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(citation_frame, text="(Seconds between citation requests)").grid(row=0, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # CITATION_TIMEOUT
        ttk.Label(citation_frame, text="Citation Timeout:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.citation_timeout_var = tk.StringVar(value="5")
        citation_timeout_entry = ttk.Entry(citation_frame, textvariable=self.citation_timeout_var, width=10)
        citation_timeout_entry.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(citation_frame, text="(Seconds)").grid(row=1, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # API Rate Limits
        api_frame = ttk.LabelFrame(scrollable_frame, text="API Rate Limits", padding=10)
        api_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # EMBEDDING_MAX_REQUESTS_PER_MINUTE
        ttk.Label(api_frame, text="Embedding Requests/Min:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.embedding_max_requests_var = tk.StringVar(value="1500")
        embedding_max_requests_entry = ttk.Entry(api_frame, textvariable=self.embedding_max_requests_var, width=10)
        embedding_max_requests_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(api_frame, text="(Google Embedding API limit)").grid(row=0, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # GEMINI_MAX_REQUESTS_PER_MINUTE
        ttk.Label(api_frame, text="Gemini Requests/Min:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.gemini_max_requests_var = tk.StringVar(value="1000")
        gemini_max_requests_entry = ttk.Entry(api_frame, textvariable=self.gemini_max_requests_var, width=10)
        gemini_max_requests_entry.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(api_frame, text="(Gemini API limit)").grid(row=1, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
    
    def create_database_config_tab(self, parent):
        """Create database configuration tab."""
        # Create scrollable frame
        canvas = tk.Canvas(parent)
        scrollbar = ttk.Scrollbar(parent, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # ChromaDB Settings
        chromadb_frame = ttk.LabelFrame(scrollable_frame, text="ChromaDB Configuration", padding=10)
        chromadb_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # DB_BATCH_SIZE
        ttk.Label(chromadb_frame, text="DB Batch Size:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.db_batch_size_var = tk.StringVar(value="5000")
        db_batch_size_entry = ttk.Entry(chromadb_frame, textvariable=self.db_batch_size_var, width=10)
        db_batch_size_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(chromadb_frame, text="(Embeddings per batch - max 5461)").grid(row=0, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Collection Name
        ttk.Label(chromadb_frame, text="Collection Name:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.collection_name_var = tk.StringVar(value="ubr5_papers")
        collection_name_entry = ttk.Entry(chromadb_frame, textvariable=self.collection_name_var, width=20)
        collection_name_entry.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(chromadb_frame, text="(ChromaDB collection name)").grid(row=1, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Persist Directory
        ttk.Label(chromadb_frame, text="Persist Directory:").grid(row=2, column=0, sticky=tk.W, pady=2)
        self.persist_directory_var = tk.StringVar(value="./chroma_db")
        persist_directory_entry = ttk.Entry(chromadb_frame, textvariable=self.persist_directory_var, width=30)
        persist_directory_entry.grid(row=2, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(chromadb_frame, text="(Database storage location)").grid(row=2, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Storage Settings
        storage_frame = ttk.LabelFrame(scrollable_frame, text="Storage Configuration", padding=10)
        storage_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # Embeddings Directory
        ttk.Label(storage_frame, text="Embeddings Directory:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.embeddings_dir_var = tk.StringVar(value="xrvix_embeddings")
        embeddings_dir_entry = ttk.Entry(storage_frame, textvariable=self.embeddings_dir_var, width=30)
        embeddings_dir_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(storage_frame, text="(Local embeddings storage)").grid(row=0, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # File Format
        ttk.Label(storage_frame, text="File Format:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.file_format_var = tk.StringVar(value="json")
        file_format_combo = ttk.Combobox(storage_frame, textvariable=self.file_format_var, width=10, state="readonly")
        file_format_combo['values'] = ('json', 'jsonl', 'csv')
        file_format_combo.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(storage_frame, text="(Data file format)").grid(row=1, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
    
    def create_api_config_tab(self, parent):
        """Create API configuration tab."""
        # Create scrollable frame
        canvas = tk.Canvas(parent)
        scrollbar = ttk.Scrollbar(parent, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Semantic Scholar API
        semantic_frame = ttk.LabelFrame(scrollable_frame, text="Semantic Scholar API", padding=10)
        semantic_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # Base URL
        ttk.Label(semantic_frame, text="Base URL:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.semantic_base_url_var = tk.StringVar(value="https://api.semanticscholar.org/graph/v1")
        semantic_base_url_entry = ttk.Entry(semantic_frame, textvariable=self.semantic_base_url_var, width=40)
        semantic_base_url_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Batch Size
        ttk.Label(semantic_frame, text="Batch Size:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.semantic_batch_size_var = tk.StringVar(value="100")
        semantic_batch_size_entry = ttk.Entry(semantic_frame, textvariable=self.semantic_batch_size_var, width=10)
        semantic_batch_size_entry.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(semantic_frame, text="(Papers per batch)").grid(row=1, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Max Papers Per Query
        ttk.Label(semantic_frame, text="Max Papers/Query:").grid(row=2, column=0, sticky=tk.W, pady=2)
        self.semantic_max_papers_var = tk.StringVar(value="50")
        semantic_max_papers_entry = ttk.Entry(semantic_frame, textvariable=self.semantic_max_papers_var, width=10)
        semantic_max_papers_entry.grid(row=2, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(semantic_frame, text="(Maximum results per search)").grid(row=2, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Embedding API
        embedding_frame = ttk.LabelFrame(scrollable_frame, text="Google Embedding API", padding=10)
        embedding_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # Model
        ttk.Label(embedding_frame, text="Model:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.embedding_model_var = tk.StringVar(value="text-embedding-004")
        embedding_model_entry = ttk.Entry(embedding_frame, textvariable=self.embedding_model_var, width=20)
        embedding_model_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(embedding_frame, text="(Google embedding model)").grid(row=0, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Max Text Length
        ttk.Label(embedding_frame, text="Max Text Length:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.embedding_max_length_var = tk.StringVar(value="8000")
        embedding_max_length_entry = ttk.Entry(embedding_frame, textvariable=self.embedding_max_length_var, width=10)
        embedding_max_length_entry.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(embedding_frame, text="(Maximum characters for embedding)").grid(row=1, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
    
    def create_search_config_tab(self, parent):
        """Create search configuration tab."""
        # Create scrollable frame
        canvas = tk.Canvas(parent)
        scrollbar = ttk.Scrollbar(parent, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Search Keywords
        keywords_frame = ttk.LabelFrame(scrollable_frame, text="Search Keywords", padding=10)
        keywords_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # PubMed Keywords
        ttk.Label(keywords_frame, text="PubMed Keywords:").grid(row=0, column=0, sticky=tk.NW, pady=2)
        self.pubmed_keywords_var = tk.StringVar(value="UBR5,ubr-5,ubr5,tumor immunology,protein degradation")
        pubmed_keywords_text = tk.Text(keywords_frame, height=3, width=50)
        pubmed_keywords_text.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        pubmed_keywords_text.insert(tk.END, self.pubmed_keywords_var.get())
        self.pubmed_keywords_text_widget = pubmed_keywords_text
        
        # Semantic Scholar Keywords
        ttk.Label(keywords_frame, text="Semantic Scholar Keywords:").grid(row=1, column=0, sticky=tk.NW, pady=2)
        self.semantic_keywords_var = tk.StringVar(value="UBR5,ubr-5,ubr5,tumor immunology,protein degradation")
        semantic_keywords_text = tk.Text(keywords_frame, height=3, width=50)
        semantic_keywords_text.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        semantic_keywords_text.insert(tk.END, self.semantic_keywords_var.get())
        self.semantic_keywords_text_widget = semantic_keywords_text
        
        # Search Settings
        search_frame = ttk.LabelFrame(scrollable_frame, text="Search Settings", padding=10)
        search_frame.pack(fill=tk.X, padx=10, pady=5)
        
        # Max Results
        ttk.Label(search_frame, text="Max Results:").grid(row=0, column=0, sticky=tk.W, pady=2)
        self.max_results_var = tk.StringVar(value="5000")
        max_results_entry = ttk.Entry(search_frame, textvariable=self.max_results_var, width=10)
        max_results_entry.grid(row=0, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(search_frame, text="(Maximum papers to collect)").grid(row=0, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Date Range
        ttk.Label(search_frame, text="Date From:").grid(row=1, column=0, sticky=tk.W, pady=2)
        self.date_from_var = tk.StringVar(value="1900")
        date_from_entry = ttk.Entry(search_frame, textvariable=self.date_from_var, width=10)
        date_from_entry.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(search_frame, text="(YYYY or YYYY-MM-DD)").grid(row=1, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        # Deduplication
        ttk.Label(search_frame, text="Enable Deduplication:").grid(row=2, column=0, sticky=tk.W, pady=2)
        self.deduplication_var = tk.BooleanVar(value=True)
        deduplication_check = ttk.Checkbutton(search_frame, variable=self.deduplication_var)
        deduplication_check.grid(row=2, column=1, sticky=tk.W, padx=(10, 0), pady=2)
        ttk.Label(search_frame, text="(Remove duplicate papers)").grid(row=2, column=2, sticky=tk.W, padx=(10, 0), pady=2)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
    
    def save_all_configurations(self):
        """Save all configuration changes to files."""
        try:
            # Save processing_config.py
            self.save_processing_config()
            
            # Save ubr5_scraper_config.py
            self.save_ubr5_scraper_config()
            
            # Save search_keywords_config.json
            self.save_search_keywords_config()
            
            messagebox.showinfo("Success", "All configurations saved successfully!")
            
        except Exception as e:
            messagebox.showerror("Error", f"Failed to save configurations: {e}")
    
    def save_processing_config(self):
        """Save processing configuration to processing_config.py."""
        config_content = f'''"""
Configuration file for optimizing embedding processing performance.
Optimized for 1 request per second (sequential processing) for maximum reliability.
Adjust these settings based on your system capabilities and API limits.
"""

# --- PARALLEL PROCESSING CONFIGURATION ---
# Optimized for sequential processing (1 req/sec) for maximum reliability
MAX_WORKERS = {self.max_workers_var.get()}  # Sequential processing (1 worker)
BATCH_SIZE = {self.batch_size_var.get()}  # Smaller batches for reliability
RATE_LIMIT_DELAY = {self.rate_limit_delay_var.get()}  # 1 second between requests
REQUEST_TIMEOUT = {self.request_timeout_var.get()}  # Request timeout in seconds
MIN_CHUNK_LENGTH = {self.min_chunk_length_var.get()}  # Minimum chunk length for text splitting
MAX_CHUNK_LENGTH = {self.max_chunk_length_var.get()}  # Maximum chunk length for text splitting
SAVE_INTERVAL = {self.save_interval_var.get()}  # Save progress every N papers

# --- CITATION PROCESSING OPTIMIZATION ---
CITATION_MAX_WORKERS = 1  # Single worker for immediate processing (no parallel needed)
CITATION_TIMEOUT = {self.citation_timeout_var.get()}  # Timeout per citation request (seconds)
CITATION_BATCH_SIZE = 1  # Process citations immediately (no batching needed)
CITATION_RATE_LIMIT = {self.citation_rate_limit_var.get()}  # 1 second between citation requests

# --- VECTOR DATABASE CONFIG ---
DB_BATCH_SIZE = {self.db_batch_size_var.get()}  # Number of embeddings to add to ChromaDB in one operation

# --- TEXT PROCESSING CONFIG ---
MIN_CHUNK_LENGTH = {self.min_chunk_length_var.get()}  # Minimum character length for a chunk
MAX_CHUNK_LENGTH = {self.max_chunk_length_var.get()}  # Maximum character length for a chunk

# --- RATE LIMITING ---
# Optimized for 1 request per second (60 requests per minute)
EMBEDDING_MAX_REQUESTS_PER_MINUTE = {self.embedding_max_requests_var.get()}  # 60 requests per minute (1 per second)
GEMINI_MAX_REQUESTS_PER_MINUTE = {self.gemini_max_requests_var.get()}     # 60 requests per minute (1 per second)
MAX_BATCH_ENQUEUED_TOKENS = 3_000_000     # Max batch enqueued tokens
RATE_LIMIT_WINDOW = 60  # Rate limiting window in seconds

# Use embedding limit for processing (since that's what we use most)
MAX_REQUESTS_PER_MINUTE = EMBEDDING_MAX_REQUESTS_PER_MINUTE

# --- MEMORY OPTIMIZATION ---
SAVE_INTERVAL = {self.save_interval_var.get()}  # Save metadata every N papers

# --- SOURCE CONFIG ---
DUMPS = ["biorxiv", "medrxiv"]  # Only process these sources

# --- PERFORMANCE PROFILES ---
PERFORMANCE_PROFILES = {{
    "sequential_optimized": {{
        "max_workers": {self.max_workers_var.get()},
        "request_timeout": {self.request_timeout_var.get()},
        "rate_limit_delay": {self.rate_limit_delay_var.get()},
        "batch_size": {self.batch_size_var.get()},
        "db_batch_size": {self.db_batch_size_var.get()}
    }}
}}

def get_config(profile="sequential_optimized"):
    """Get configuration for the sequential optimized processing profile."""
    return PERFORMANCE_PROFILES["sequential_optimized"]

def print_config_info():
    """Print current configuration information."""
    print("üîß Current Processing Configuration (Sequential Optimized):")
    print(f"   Processing mode: Sequential ({{MAX_WORKERS}} worker)")
    print(f"   Request timeout: {{REQUEST_TIMEOUT}}s")
    print(f"   Batch size: {{BATCH_SIZE}}")
    print(f"   Rate limit delay: {{RATE_LIMIT_DELAY}}s (1 request per second)")
    print(f"   Sources: {{', '.join(DUMPS)}}")
    print()
    print("üìä Citation Processing:")
    print(f"   Citation workers: {{CITATION_MAX_WORKERS}} (immediate processing)")
    print(f"   Citation timeout: {{CITATION_TIMEOUT}}s")
    print(f"   Citation rate limit: {{CITATION_RATE_LIMIT}}s between requests")
    print()
    print("üí° Performance Notes:")
    print("   - Sequential processing for maximum reliability")
    print("   - 1 request per second to avoid rate limits")
    print("   - Smaller batch sizes for better error recovery")

if __name__ == "__main__":
    print_config_info()
'''
        
        with open("src/core/processing_config.py", "w", encoding="utf-8") as f:
            f.write(config_content)
    
    def save_ubr5_scraper_config(self):
        """Save UBR5 scraper configuration to ubr5_scraper_config.py."""
        config_content = f'''"""
Configuration file for the UBR5 API scraper.
Contains all configurable parameters for paper collection and processing.
"""

# --- UBR5 SEARCH CONFIGURATION ---
UBR5_SEARCH_TERMS = [
    "UBR5", "ubr5", "Ubr5", "ubiquitin protein ligase E3 component n-recognin 5",
    "EDD1", "edd1", "Edd1", "E3 ubiquitin-protein ligase UBR5",
    "ubiquitin ligase UBR5", "UBR5 function", "UBR5 regulation", 
    "UBR5 expression", "UBR5 mutation", "UBR5 activity", "UBR5 mechanism",
    "UBR5 cancer", "UBR5 disease", "UBR5 pathway", "UBR5 oncology",
    "UBR5 tumor", "UBR5 metastasis", "UBR5 interaction", "UBR5 binding",
    "UBR5 complex", "UBR5 protein-protein", "UBR5 substrate",
    "UBR5 ubiquitination", "UBR5 degradation", "UBR5 signaling",
    "UBR5 transcription", "UBR5 DNA repair", "UBR5 tissue",
    "UBR5 cell line", "UBR5 organ", "UBR5 development"
]

# --- API CONFIGURATION ---
SEMANTIC_SCHOLAR_CONFIG = {{
    "base_url": "{self.semantic_base_url_var.get()}",
    "search_endpoint": "/paper/search",
    "paper_endpoint": "/paper",
    "batch_size": {self.semantic_batch_size_var.get()},
    "max_papers_per_query": {self.semantic_max_papers_var.get()},
    "fields": [
        "paperId", "title", "abstract", "venue", "year", "authors",
        "referenceCount", "citationCount", "openAccessPdf", 
        "publicationDate", "publicationTypes", "fieldsOfStudy",
        "publicationVenue", "externalIds", "url", "isOpenAccess"
    ]
}}

SCHOLARLY_CONFIG = {{
    "max_papers_per_query": 30,
    "search_keywords": ["ubr5", "UBR5", "ubr-5", "UBR-5"]
}}

# --- RATE LIMITING ---
# Optimized for 1 request per second for maximum reliability
RATE_LIMITING = {{
    "semantic_scholar_delay": 1.0,  # 1 second between Semantic Scholar requests
    "scholarly_delay": 1.0,         # 1 second between scholarly requests
    "keyword_delay": 1.0,           # 1 second between search keywords
    "embedding_delay": 1.0,         # 1 second between embedding requests
    "max_retries": 3,
    "timeout": 30,
    "rate_limit_wait": 60
}}

# --- EMBEDDING CONFIGURATION ---
EMBEDDING_CONFIG = {{
    "model": "{self.embedding_model_var.get()}",
    "api_endpoint": "https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:embedText",
    "max_text_length": {self.embedding_max_length_var.get()},
    "text_components": [
        "title", "abstract", "authors", "journal", "year", "fields_of_study"
    ]
}}

# --- PAPER PROCESSING ---
PAPER_PROCESSING = {{
    "min_title_length": 10,
    "min_abstract_length": 50,
    "title_similarity_threshold": 0.8,
    "max_abstract_length": 5000,
    "required_fields": ["title"],
    "optional_fields": ["abstract", "authors", "journal", "year", "doi"]
}}

# --- STORAGE CONFIGURATION ---
STORAGE_CONFIG = {{
    "embeddings_dir": "{self.embeddings_dir_var.get()}",
    "source_name": "ubr5_api",
    "file_format": "{self.file_format_var.get()}",
    "metadata_file": "metadata.json",
    "max_filename_length": 100,
    "save_individual_files": True,
    "save_metadata": True
}}

# --- CHROMADB INTEGRATION ---
CHROMADB_CONFIG = {{
    "collection_name": "{self.collection_name_var.get()}",
    "persist_directory": "{self.persist_directory_var.get()}",
    "metadata_fields": [
        "title", "doi", "authors", "journal", "year", "citation_count",
        "source", "is_preprint", "publication_date", "fields_of_study",
        "publication_types", "abstract"
    ],
    "max_abstract_length": 1000,
    "id_prefix": "ubr5_api"
}}

# --- SEARCH CONFIGURATION ---
SEARCH_CONFIG = {{
    "comprehensive": {{
        "max_papers": {self.max_results_var.get()},
        "use_semantic_scholar": True,
        "use_scholarly": True,
        "search_all_keywords": True,
        "deduplication": {str(self.deduplication_var.get()).lower()}
    }},
    "focused": {{
        "max_papers": 500,
        "use_semantic_scholar": True,
        "use_scholarly": False,
        "search_all_keywords": False,
        "deduplication": True
    }},
    "quick": {{
        "max_papers": 200,
        "use_semantic_scholar": True,
        "use_scholarly": False,
        "search_all_keywords": False,
        "deduplication": False
    }}
}}

# --- LOGGING CONFIGURATION ---
LOGGING_CONFIG = {{
    "level": "INFO",
    "format": "%(asctime)s - %(levelname)s - %(message)s",
    "file": "ubr5_api_scraping.log",
    "console": True,
    "file_handler": True
}}

# --- PERFORMANCE PROFILES ---
PERFORMANCE_PROFILES = {{
    "sequential_optimized": {{
        "rate_limit_delay": 1.0,  # 1 second between requests
        "strategy_delay": 1.0,    # 1 second between strategies
        "max_workers": 1,         # Sequential processing
        "batch_size": 100         # Smaller batches for reliability
    }},
    "conservative": {{
        "rate_limit_delay": 2.0,  # 2 seconds between requests
        "strategy_delay": 2.0,
        "max_workers": 1,
        "batch_size": 50
    }},
    "balanced": {{
        "rate_limit_delay": 0.5,  # 0.5 seconds between requests
        "strategy_delay": 1.0,
        "max_workers": 2,
        "batch_size": 100
    }},
    "aggressive": {{
        "rate_limit_delay": 0.1,  # 0.1 seconds between requests (may hit rate limits)
        "strategy_delay": 0.5,
        "max_workers": 4,
        "batch_size": 200
    }}
}}

def get_config(profile: str = "sequential_optimized") -> dict:
    """Get configuration for a specific performance profile."""
    if profile not in PERFORMANCE_PROFILES:
        profile = "sequential_optimized"
    
    config = PERFORMANCE_PROFILES[profile].copy()
    config.update({{
        "search_keywords": SCHOLARLY_CONFIG["search_keywords"],
        "semantic_scholar": SEMANTIC_SCHOLAR_CONFIG,
        "scholarly": SCHOLARLY_CONFIG,
        "rate_limiting": RATE_LIMITING,
        "embedding": EMBEDDING_CONFIG,
        "paper_processing": PAPER_PROCESSING,
        "storage": STORAGE_CONFIG,
        "chromadb": CHROMADB_CONFIG,
        "logging": LOGGING_CONFIG
    }})
    
    return config

def print_config_info(profile: str = "sequential_optimized"):
    """Print current configuration information."""
    config = get_config(profile)
    
    print("üîß UBR5 Scraper Configuration (Sequential Optimized):")
    print(f"   Performance profile: {{profile}}")
    print(f"   Processing mode: Sequential (1 worker)")
    print(f"   Rate limit delay: {{config['rate_limiting']['semantic_scholar_delay']}}s (1 request per second)")
    print(f"   Keyword delay: {{config['rate_limiting']['keyword_delay']}}s")
    print(f"   Search keywords: {{len(config['search_keywords'])}} keywords")
    print(f"   Embedding model: {{config['embedding']['model']}}")
    print(f"   Storage directory: {{config['storage']['embeddings_dir']}}")
    print(f"   ChromaDB collection: {{config['chromadb']['collection_name']}}")
    print()
    print("üí° Performance Notes:")
    print("   - Sequential processing for maximum reliability")
    print("   - 1 request per second to avoid rate limits")
    print("   - Optimized for stable, long-running operations")

if __name__ == "__main__":
    print_config_info()
'''
        
        with open("ubr5_scraper_config.py", "w", encoding="utf-8") as f:
            f.write(config_content)
    
    def save_search_keywords_config(self):
        """Save search keywords configuration to search_keywords_config.json."""
        config_data = {
            "pubmed_keywords": self.pubmed_keywords_text_widget.get("1.0", tk.END).strip(),
            "semantic_keywords": self.semantic_keywords_text_widget.get("1.0", tk.END).strip(),
            "last_updated": {"timestamp": datetime.now().isoformat()}
        }
        
        with open("config/search_keywords_config.json", "w", encoding="utf-8") as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
    
    def load_all_configurations(self):
        """Load all configurations from files."""
        try:
            # Load processing_config.py
            self.load_processing_config()
            
            # Load ubr5_scraper_config.py
            self.load_ubr5_scraper_config()
            
            # Load search_keywords_config.json
            self.load_search_keywords_config()
            
            messagebox.showinfo("Success", "All configurations loaded successfully!")
            
        except Exception as e:
            messagebox.showerror("Error", f"Failed to load configurations: {e}")
    
    def load_processing_config(self):
        """Load processing configuration from processing_config.py."""
        try:
            from src.core import processing_config
            
            # Load values from the imported module
            self.max_workers_var.set(str(processing_config.MAX_WORKERS))
            self.batch_size_var.set(str(processing_config.BATCH_SIZE))
            self.rate_limit_delay_var.set(str(processing_config.RATE_LIMIT_DELAY))
            self.request_timeout_var.set(str(processing_config.REQUEST_TIMEOUT))
            self.min_chunk_length_var.set(str(processing_config.MIN_CHUNK_LENGTH))
            self.max_chunk_length_var.set(str(processing_config.MAX_CHUNK_LENGTH))
            self.save_interval_var.set(str(processing_config.SAVE_INTERVAL))
            self.citation_rate_limit_var.set(str(processing_config.CITATION_RATE_LIMIT))
            self.citation_timeout_var.set(str(processing_config.CITATION_TIMEOUT))
            self.db_batch_size_var.set(str(processing_config.DB_BATCH_SIZE))
            self.embedding_max_requests_var.set(str(processing_config.EMBEDDING_MAX_REQUESTS_PER_MINUTE))
            self.gemini_max_requests_var.set(str(processing_config.GEMINI_MAX_REQUESTS_PER_MINUTE))
            
        except Exception as e:
            print(f"Error loading processing config: {e}")
    
    def load_ubr5_scraper_config(self):
        """Load UBR5 scraper configuration from ubr5_scraper_config.py."""
        try:
            from src.scrapers import ubr5_scraper_config
            
            # Load values from the imported module
            self.semantic_base_url_var.set(ubr5_scraper_config.SEMANTIC_SCHOLAR_CONFIG["base_url"])
            self.semantic_batch_size_var.set(str(ubr5_scraper_config.SEMANTIC_SCHOLAR_CONFIG["batch_size"]))
            self.semantic_max_papers_var.set(str(ubr5_scraper_config.SEMANTIC_SCHOLAR_CONFIG["max_papers_per_query"]))
            self.embedding_model_var.set(ubr5_scraper_config.EMBEDDING_CONFIG["model"])
            self.embedding_max_length_var.set(str(ubr5_scraper_config.EMBEDDING_CONFIG["max_text_length"]))
            self.collection_name_var.set(ubr5_scraper_config.CHROMADB_CONFIG["collection_name"])
            self.persist_directory_var.set(ubr5_scraper_config.CHROMADB_CONFIG["persist_directory"])
            self.embeddings_dir_var.set(ubr5_scraper_config.STORAGE_CONFIG["embeddings_dir"])
            self.file_format_var.set(ubr5_scraper_config.STORAGE_CONFIG["file_format"])
            
        except Exception as e:
            print(f"Error loading UBR5 scraper config: {e}")
    
    def load_search_keywords_config(self):
        """Load search keywords configuration from search_keywords_config.json."""
        try:
            with open("config/search_keywords_config.json", "r", encoding="utf-8") as f:
                config_data = json.load(f)
            
            # Load keywords into text widgets
            self.pubmed_keywords_text_widget.delete("1.0", tk.END)
            self.pubmed_keywords_text_widget.insert("1.0", config_data.get("pubmed_keywords", ""))
            
            self.semantic_keywords_text_widget.delete("1.0", tk.END)
            self.semantic_keywords_text_widget.insert("1.0", config_data.get("semantic_keywords", ""))
            
        except Exception as e:
            print(f"Error loading search keywords config: {e}")
    
    def reset_configurations(self):
        """Reset all configurations to default values."""
        if messagebox.askyesno("Confirm Reset", "Are you sure you want to reset all configurations to default values? This cannot be undone."):
            try:
                # Reset to optimized default values (1 req/sec)
                self.max_workers_var.set("1")  # Sequential processing
                self.batch_size_var.set("100")  # Smaller batches for reliability
                self.rate_limit_delay_var.set("1.0")  # 1 second between requests
                self.request_timeout_var.set("30")
                self.min_chunk_length_var.set("50")
                self.max_chunk_length_var.set("8000")
                self.save_interval_var.set("1000")
                self.citation_rate_limit_var.set("1.0")  # 1 second between citation requests
                self.citation_timeout_var.set("5")
                self.db_batch_size_var.set("5000")
                self.embedding_max_requests_var.set("60")  # 60 requests per minute (1 per second)
                self.gemini_max_requests_var.set("60")  # 60 requests per minute (1 per second)
                
                self.semantic_base_url_var.set("https://api.semanticscholar.org/graph/v1")
                self.semantic_batch_size_var.set("100")
                self.semantic_max_papers_var.set("50")
                self.embedding_model_var.set("text-embedding-004")
                self.embedding_max_length_var.set("8000")
                self.collection_name_var.set("ubr5_papers")
                self.persist_directory_var.set("./chroma_db")
                self.embeddings_dir_var.set("xrvix_embeddings")
                self.file_format_var.set("json")
                
                self.max_results_var.set("5000")
                self.date_from_var.set("1900")
                self.deduplication_var.set(True)
                
                self.pubmed_keywords_text_widget.delete("1.0", tk.END)
                self.pubmed_keywords_text_widget.insert("1.0", "UBR5,ubr-5,ubr5,tumor immunology,protein degradation")
                
                self.semantic_keywords_text_widget.delete("1.0", tk.END)
                self.semantic_keywords_text_widget.insert("1.0", "UBR5,ubr-5,ubr5,tumor immunology,protein degradation")
                
                messagebox.showinfo("Success", "All configurations reset to default values!")
                
            except Exception as e:
                messagebox.showerror("Error", f"Failed to reset configurations: {e}")
    
    def init_config_variables(self):
        """Initialize configuration variables with optimized default values."""
        # Processing configuration variables - optimized for 1 req/sec
        self.max_workers_var = tk.StringVar(value="1")  # Sequential processing
        self.batch_size_var = tk.StringVar(value="100")  # Smaller batches for reliability
        self.rate_limit_delay_var = tk.StringVar(value="1.0")  # 1 second between requests
        self.request_timeout_var = tk.StringVar(value="30")
        self.min_chunk_length_var = tk.StringVar(value="50")
        self.max_chunk_length_var = tk.StringVar(value="8000")
        self.save_interval_var = tk.StringVar(value="1000")
        self.citation_rate_limit_var = tk.StringVar(value="1.0")  # 1 second between citation requests
        self.citation_timeout_var = tk.StringVar(value="5")
        self.db_batch_size_var = tk.StringVar(value="5000")
        self.embedding_max_requests_var = tk.StringVar(value="60")  # 60 requests per minute (1 per second)
        self.gemini_max_requests_var = tk.StringVar(value="60")  # 60 requests per minute (1 per second)
        
        # API configuration variables
        self.semantic_base_url_var = tk.StringVar(value="https://api.semanticscholar.org/graph/v1")
        self.semantic_batch_size_var = tk.StringVar(value="100")
        self.semantic_max_papers_var = tk.StringVar(value="50")
        self.embedding_model_var = tk.StringVar(value="text-embedding-004")
        self.embedding_max_length_var = tk.StringVar(value="8000")
        
        # Database configuration variables
        self.collection_name_var = tk.StringVar(value="ubr5_papers")
        self.persist_directory_var = tk.StringVar(value="./chroma_db")
        self.embeddings_dir_var = tk.StringVar(value="xrvix_embeddings")
        self.file_format_var = tk.StringVar(value="json")
        
        # Search configuration variables
        self.max_results_var = tk.StringVar(value="5000")
        self.date_from_var = tk.StringVar(value="1900")
        self.deduplication_var = tk.BooleanVar(value=True)
        self.pubmed_keywords_var = tk.StringVar(value="UBR5,ubr-5,ubr5,tumor immunology,protein degradation")
        self.semantic_keywords_var = tk.StringVar(value="UBR5,ubr-5,ubr5,tumor immunology,protein degradation")
        
        # Initialize text widgets as None (will be set when tabs are created)
        self.pubmed_keywords_text_widget = None
        self.semantic_keywords_text_widget = None
        
    def create_hypothesis_tab(self):
        """Create Hypothesis Generation tab with subtabs."""
        # Create notebook for subtabs
        hypothesis_notebook = ttk.Notebook(self.hypothesis_tab)
        hypothesis_notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Generate Hypotheses subtab
        generate_frame = ttk.Frame(hypothesis_notebook)
        hypothesis_notebook.add(generate_frame, text="Generate Hypotheses")
        self.create_generate_hypotheses_subtab(generate_frame)
        
        # Test Run subtab
        test_frame = ttk.Frame(hypothesis_notebook)
        hypothesis_notebook.add(test_frame, text="Test Run")
        self.create_test_run_subtab(test_frame)
        
    def create_generate_hypotheses_subtab(self, parent):
        """Create generate hypotheses subtab with scrollable content."""
        # Create a canvas and scrollbar for the entire tab
        canvas = tk.Canvas(parent)
        scrollbar = ttk.Scrollbar(parent, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Pack canvas and scrollbar
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        # Bind mousewheel to canvas
        def _on_mousewheel(event):
            canvas.yview_scroll(int(-1*(event.delta/120)), "units")
        canvas.bind_all("<MouseWheel>", _on_mousewheel)
        
        # Title
        title_label = ttk.Label(scrollable_frame, text="Generate Hypotheses", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """This option launches the Enhanced RAG System for hypothesis generation.
The system will use the loaded ChromaDB data to generate novel research hypotheses."""
        
        desc_label = ttk.Label(scrollable_frame, text=desc_text, justify=tk.LEFT)
        desc_label.pack(pady=(0, 20))
        
        # Input fields frame
        input_frame = ttk.LabelFrame(scrollable_frame, text="Hypothesis Generation Parameters", padding=10)
        input_frame.pack(fill=tk.X, pady=(0, 20))
        
        # Prompt input
        prompt_label = ttk.Label(input_frame, text="Research Prompt:")
        prompt_label.pack(anchor=tk.W, pady=(0, 5))
        
        self.hypothesis_prompt = tk.Text(
            input_frame, 
            height=4,  # Increased from 3 to 4
            wrap=tk.WORD,
            font=('Arial', 10)  # Better font for input
        )
        self.hypothesis_prompt.pack(fill=tk.X, pady=(0, 10))
        
        # Add placeholder text
        placeholder_text = "Enter your research question or hypothesis prompt here...\nExample: How does UBR5 regulate cancer immunity through protein ubiquitination?"
        self.hypothesis_prompt.insert("1.0", placeholder_text)
        self.hypothesis_prompt.config(fg='gray')
        
        # Bind events to handle placeholder text
        self.hypothesis_prompt.bind("<FocusIn>", self._on_prompt_focus_in)
        self.hypothesis_prompt.bind("<FocusOut>", self._on_prompt_focus_out)
        
        # Lab and Institution inputs in a row
        lab_inst_frame = ttk.Frame(input_frame)
        lab_inst_frame.pack(fill=tk.X, pady=(0, 10))
        
        # Lab input
        lab_label = ttk.Label(lab_inst_frame, text="Lab Name:")
        lab_label.pack(side=tk.LEFT, padx=(0, 10))
        
        self.hypothesis_lab = tk.StringVar(value="Dr. Xiaojing Ma")
        lab_entry = ttk.Entry(lab_inst_frame, textvariable=self.hypothesis_lab, width=25)
        lab_entry.pack(side=tk.LEFT, padx=(0, 20))
        
        # Institution input
        inst_label = ttk.Label(lab_inst_frame, text="Institution:")
        inst_label.pack(side=tk.LEFT, padx=(0, 10))
        
        self.hypothesis_institution = tk.StringVar(value="Weill Cornell Medicine")
        inst_entry = ttk.Entry(lab_inst_frame, textvariable=self.hypothesis_institution, width=25)
        inst_entry.pack(side=tk.LEFT)
        
        # Hypotheses per meta-hypothesis input
        hypotheses_per_meta_frame = ttk.Frame(input_frame)
        hypotheses_per_meta_frame.pack(fill=tk.X, pady=(0, 10))
        
        hypotheses_per_meta_label = ttk.Label(hypotheses_per_meta_frame, text="Hypotheses per Meta-Hypothesis:")
        hypotheses_per_meta_label.pack(side=tk.LEFT, padx=(0, 10))
        
        self.hypotheses_per_meta = tk.StringVar(value="3")
        hypotheses_per_meta_entry = ttk.Entry(hypotheses_per_meta_frame, textvariable=self.hypotheses_per_meta, width=10)
        hypotheses_per_meta_entry.pack(side=tk.LEFT, padx=(0, 10))
        
        # Add help text
        help_text = ttk.Label(hypotheses_per_meta_frame, text="(Number of accepted hypotheses to generate for each meta-hypothesis)", 
                             font=('Arial', 8), foreground='gray')
        help_text.pack(side=tk.LEFT)
        
        # Prerequisites check
        prereq_frame = ttk.LabelFrame(scrollable_frame, text="Prerequisites Check", padding=10)
        prereq_frame.pack(fill=tk.X, pady=(0, 20))
        
        self.prereq_status_label = ttk.Label(prereq_frame, text="Checking prerequisites...")
        self.prereq_status_label.pack(anchor=tk.W)
        
        # Buttons
        button_frame = ttk.Frame(scrollable_frame)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        check_prereq_button = ttk.Button(button_frame, text="üîç Check Prerequisites", 
                                       command=self.check_hypothesis_prerequisites)
        check_prereq_button.pack(side=tk.LEFT, padx=(0, 10))
        
        generate_button = ttk.Button(button_frame, text="üß† Generate Hypotheses", 
                                   command=self.generate_hypotheses_gui, style='Action.TButton')
        generate_button.pack(side=tk.LEFT, padx=(0, 10))
        
        clear_output_button = ttk.Button(button_frame, text="üóëÔ∏è Clear Output", 
                                       command=self.clear_hypothesis_output)
        clear_output_button.pack(side=tk.LEFT)
        
        # Progress bar area
        progress_frame = ttk.LabelFrame(scrollable_frame, text="Progress", padding=10)
        progress_frame.pack(fill=tk.X, pady=(0, 10))
        
        # Progress step label
        self.progress_step_label = ttk.Label(progress_frame, text="Ready to generate hypotheses", 
                                           font=('Arial', 10, 'bold'))
        self.progress_step_label.pack(anchor=tk.W, pady=(0, 5))
        
        # Progress bar
        self.progress_bar = ttk.Progressbar(progress_frame, mode='indeterminate', length=400)
        self.progress_bar.pack(fill=tk.X, pady=(0, 5))
        
        # Progress percentage label
        self.progress_percent_label = ttk.Label(progress_frame, text="0%", font=('Arial', 9))
        self.progress_percent_label.pack(anchor=tk.W)
        
        # Output area - Much larger now
        output_frame = ttk.LabelFrame(scrollable_frame, text="Output", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 20))
        
        # Create a much larger output text area with better scrolling
        self.hypothesis_output = scrolledtext.ScrolledText(
            output_frame, 
            height=40,  # Significantly increased from 25 to 40
            width=120,  # Increased width from 100 to 120
            wrap=tk.WORD,  # Word wrapping
            font=('Consolas', 9)  # Slightly smaller font to fit more content
        )
        self.hypothesis_output.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # Check prerequisites on load
        self.root.after(100, self.check_hypothesis_prerequisites)
    
    def _on_prompt_focus_in(self, event):
        """Handle focus in event for prompt text area."""
        if self.hypothesis_prompt.get("1.0", tk.END).strip() == "Enter your research question or hypothesis prompt here...\nExample: How does UBR5 regulate cancer immunity through protein ubiquitination?":
            self.hypothesis_prompt.delete("1.0", tk.END)
            self.hypothesis_prompt.config(fg='black')
    
    def _on_prompt_focus_out(self, event):
        """Handle focus out event for prompt text area."""
        if not self.hypothesis_prompt.get("1.0", tk.END).strip():
            placeholder_text = "Enter your research question or hypothesis prompt here...\nExample: How does UBR5 regulate cancer immunity through protein ubiquitination?"
            self.hypothesis_prompt.insert("1.0", placeholder_text)
            self.hypothesis_prompt.config(fg='gray')
    
    def clear_hypothesis_output(self):
        """Clear the hypothesis output area."""
        self.hypothesis_output.delete("1.0", tk.END)
        self.log_message(self.hypothesis_output, "üóëÔ∏è Output cleared")
        self.update_progress("Ready to generate hypotheses", 0, stop_animation=True)
    
    def update_progress(self, step_text, percentage=None, start_animation=False, stop_animation=False):
        """Update the progress bar and step text."""
        # Update step text
        self.progress_step_label.config(text=step_text)
        
        # Update percentage if provided
        if percentage is not None:
            self.progress_percent_label.config(text=f"{percentage}%")
        
        # Control progress bar animation
        if start_animation:
            self.progress_bar.config(mode='indeterminate')
            self.progress_bar.start(10)  # Start animation
        elif stop_animation:
            self.progress_bar.stop()
            self.progress_bar.config(mode='determinate', value=0)
        elif percentage is not None:
            self.progress_bar.config(mode='determinate', value=percentage)
        
        # Force GUI update
        self.root.update_idletasks()
    
    def log_progress(self, step_text, percentage=None, start_animation=False, stop_animation=False):
        """Log progress to both terminal and GUI output."""
        # Update progress bar
        self.update_progress(step_text, percentage, start_animation, stop_animation)
        
        # Log to GUI output
        self.log_message(self.hypothesis_output, f"üìä {step_text}")
        
        # Log to terminal
        print(f"üìä {step_text}")
        
        # Force GUI update
        self.root.update_idletasks()
        
    def create_test_run_subtab(self, parent):
        """Create test run subtab."""
        # Title
        title_label = ttk.Label(parent, text="Test Run: Biomedical Research", 
                               font=('Arial', 14, 'bold'))
        title_label.pack(pady=(0, 20))
        
        # Description
        desc_text = """This option runs a test query focused on biomedical research.
It's useful for testing the system with a specific research focus."""
        
        desc_label = ttk.Label(parent, text=desc_text, justify=tk.LEFT)
        desc_label.pack(pady=(0, 20))
        
        # Test query configuration
        query_frame = ttk.LabelFrame(parent, text="Test Query Configuration", padding=10)
        query_frame.pack(fill=tk.X, pady=(0, 20))
        
        ttk.Label(query_frame, text="Test Query:").pack(anchor=tk.W)
        self.test_query_var = tk.StringVar(value="protein function and disease")
        test_query_entry = ttk.Entry(query_frame, textvariable=self.test_query_var, width=60)
        test_query_entry.pack(fill=tk.X, pady=(0, 10))
        
        # Prerequisites check
        prereq_frame = ttk.LabelFrame(parent, text="Prerequisites Check", padding=10)
        prereq_frame.pack(fill=tk.X, pady=(0, 20))
        
        self.test_prereq_status_label = ttk.Label(prereq_frame, text="Checking prerequisites...")
        self.test_prereq_status_label.pack(anchor=tk.W)
        
        # Buttons
        button_frame = ttk.Frame(parent)
        button_frame.pack(fill=tk.X, pady=(0, 20))
        
        check_prereq_button = ttk.Button(button_frame, text="üîç Check Prerequisites", 
                                       command=self.check_test_prerequisites)
        check_prereq_button.pack(side=tk.LEFT, padx=(0, 10))
        
        test_button = ttk.Button(button_frame, text="üß™ Run Test", 
                               command=self.test_run_gui, style='Action.TButton')
        test_button.pack(side=tk.LEFT)
        
        # Output area
        output_frame = ttk.LabelFrame(parent, text="Output", padding=10)
        output_frame.pack(fill=tk.BOTH, expand=True)
        
        self.test_output = scrolledtext.ScrolledText(output_frame, height=15)
        self.test_output.pack(fill=tk.BOTH, expand=True)
        
        # Check prerequisites on load
        self.root.after(100, self.check_test_prerequisites)
        
    def create_tutorial_tab(self):
        """Create Tutorial tab with data pipeline overview."""
        # Create scrollable frame for tutorial content
        canvas = tk.Canvas(self.tutorial_tab)
        scrollbar = ttk.Scrollbar(self.tutorial_tab, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # Pack canvas and scrollbar
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        # Tutorial content
        self.create_tutorial_content(scrollable_frame)
        
        # Bind mousewheel to canvas
        def _on_mousewheel(event):
            canvas.yview_scroll(int(-1*(event.delta/120)), "units")
        canvas.bind_all("<MouseWheel>", _on_mousewheel)
        
    def create_tutorial_content(self, parent):
        """Create tutorial content."""
        # Title
        title_label = ttk.Label(parent, text="AI Research Processor Tutorial", 
                               font=('Arial', 18, 'bold'))
        title_label.pack(pady=(20, 30))
        
        # Introduction
        intro_frame = ttk.LabelFrame(parent, text="Introduction", padding=15)
        intro_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        intro_text = """Welcome to the AI Research Processor! This system is designed to help researchers 
generate novel scientific hypotheses by analyzing vast amounts of scientific literature for any biomedical research topic.

The system uses AI-powered techniques to:
‚Ä¢ Scrape and process scientific papers from multiple sources
‚Ä¢ Generate embeddings for semantic search
‚Ä¢ Use advanced RAG (Retrieval-Augmented Generation) to create hypotheses
‚Ä¢ Provide insights into biomedical research and scientific discovery"""
        
        intro_label = ttk.Label(intro_frame, text=intro_text, justify=tk.LEFT, wraplength=800)
        intro_label.pack(anchor=tk.W)
        
        # Data Pipeline Overview
        pipeline_frame = ttk.LabelFrame(parent, text="Data Pipeline Overview", padding=15)
        pipeline_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        pipeline_text = """The AI Research Processor follows a structured data pipeline with 4 main stages:

1. üìö DATA COLLECTION - Scrape scientific papers from multiple sources
2. üîÑ DATA PROCESSING - Generate embeddings and process content
3. üóÑÔ∏è VECTOR STORAGE - Store processed data in ChromaDB
4. üß† HYPOTHESIS GENERATION - Use AI to generate research hypotheses

Each stage builds upon the previous one, creating a comprehensive research analysis system."""
        
        pipeline_label = ttk.Label(pipeline_frame, text=pipeline_text, justify=tk.LEFT, wraplength=800)
        pipeline_label.pack(anchor=tk.W)
        
        # Stage 1: Data Collection
        stage1_frame = ttk.LabelFrame(parent, text="Stage 1: Data Collection üìö", padding=15)
        stage1_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        stage1_text = """The system collects scientific papers from three main sources:

üî¨ PubMed (Journal Articles)
‚Ä¢ Source: National Library of Medicine's PubMed database
‚Ä¢ Content: Peer-reviewed journal articles
‚Ä¢ Keywords: Customizable search terms (default: biomedical research terms)
‚Ä¢ Processing: Direct API access with configurable result limits

üìÑ Preprints (Biorxiv, Medrxiv)
‚Ä¢ Source: Preprint servers for biology and medicine
‚Ä¢ Content: Latest research before peer review
‚Ä¢ Processing: Bulk download and processing of dump files
‚Ä¢ Advantage: Access to cutting-edge research

üîç Semantic Scholar API
‚Ä¢ Source: AI-powered academic search engine
‚Ä¢ Content: Comprehensive academic literature
‚Ä¢ Keywords: Customizable search terms
‚Ä¢ Processing: Advanced citation analysis and metadata extraction

üí° TIP: Start with the "Full Scraper" option to collect data from all sources at once."""
        
        stage1_label = ttk.Label(stage1_frame, text=stage1_text, justify=tk.LEFT, wraplength=800)
        stage1_label.pack(anchor=tk.W)
        
        # Stage 2: Data Processing
        stage2_frame = ttk.LabelFrame(parent, text="Stage 2: Data Processing üîÑ", padding=15)
        stage2_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        stage2_text = """Once papers are collected, the system processes them for analysis:

üìù Text Extraction
‚Ä¢ Extract full text content from papers
‚Ä¢ Clean and normalize text data
‚Ä¢ Handle different document formats (PDF, HTML, etc.)

üî§ Embedding Generation
‚Ä¢ Convert text into numerical vectors (embeddings)
‚Ä¢ Use advanced language models for semantic understanding
‚Ä¢ Enable similarity search and clustering

üìä Metadata Processing
‚Ä¢ Extract key information: authors, journals, citations, impact factors
‚Ä¢ Process abstracts, keywords, and references
‚Ä¢ Organize data by source and topic

üè∑Ô∏è Categorization
‚Ä¢ Group papers by research area
‚Ä¢ Identify key themes and concepts
‚Ä¢ Prepare data for vector storage

üí° TIP: Embeddings are automatically generated during the scraping process."""
        
        stage2_label = ttk.Label(stage2_frame, text=stage2_text, justify=tk.LEFT, wraplength=800)
        stage2_label.pack(anchor=tk.W)
        
        # Stage 3: Vector Storage
        stage3_frame = ttk.LabelFrame(parent, text="Stage 3: Vector Storage üóÑÔ∏è", padding=15)
        stage3_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        stage3_text = """Processed data is stored in ChromaDB, a vector database:

üíæ ChromaDB Features
‚Ä¢ Persistent storage of embeddings and metadata
‚Ä¢ Fast similarity search capabilities
‚Ä¢ Automatic indexing and optimization
‚Ä¢ Support for large-scale data collections

üì¶ Data Organization
‚Ä¢ Collections organized by data source
‚Ä¢ Batch processing for efficient loading
‚Ä¢ Source tracking and statistics
‚Ä¢ Easy data management and updates

üîç Search Capabilities
‚Ä¢ Semantic similarity search
‚Ä¢ Keyword-based filtering
‚Ä¢ Citation network analysis
‚Ä¢ Temporal analysis of research trends

üìä Monitoring
‚Ä¢ Real-time statistics on stored documents
‚Ä¢ Source breakdown and analysis
‚Ä¢ Data quality metrics
‚Ä¢ Storage optimization

üí° TIP: Use "Load Embeddings" to populate ChromaDB with your collected data."""
        
        stage3_label = ttk.Label(stage3_frame, text=stage3_text, justify=tk.LEFT, wraplength=800)
        stage3_label.pack(anchor=tk.W)
        
        # Stage 4: Hypothesis Generation
        stage4_frame = ttk.LabelFrame(parent, text="Stage 4: Hypothesis Generation üß†", padding=15)
        stage4_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        stage4_text = """The final stage uses AI to generate novel research hypotheses:

ü§ñ RAG System (Retrieval-Augmented Generation)
‚Ä¢ Combines retrieval of relevant papers with AI generation
‚Ä¢ Uses advanced language models for hypothesis creation
‚Ä¢ Context-aware analysis of research gaps

üîç Knowledge Synthesis
‚Ä¢ Identifies patterns across multiple papers
‚Ä¢ Connects disparate research findings
‚Ä¢ Highlights unexplored research directions

üí° Hypothesis Types
‚Ä¢ Mechanistic hypotheses about protein function
‚Ä¢ Therapeutic intervention strategies
‚Ä¢ Biomarker discovery opportunities
‚Ä¢ Novel research methodologies

üéØ Focus Areas
‚Ä¢ Protein function and regulation
‚Ä¢ Disease biology and immunology
‚Ä¢ Cellular pathways and systems
‚Ä¢ Therapeutic target identification

üí° TIP: Start with the "Test Run" to see how hypothesis generation works."""
        
        stage4_label = ttk.Label(stage4_frame, text=stage4_text, justify=tk.LEFT, wraplength=800)
        stage4_label.pack(anchor=tk.W)
        
        # Workflow Guide
        workflow_frame = ttk.LabelFrame(parent, text="Recommended Workflow üöÄ", padding=15)
        workflow_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        workflow_text = """Follow this step-by-step workflow for best results:

STEP 1: Check System Status
‚Ä¢ Go to Settings ‚Üí Data Status
‚Ä¢ Verify what data sources are available
‚Ä¢ Check ChromaDB status

STEP 2: Collect Data (if needed)
‚Ä¢ Use Paper Scraping ‚Üí Full Scraper for comprehensive collection
‚Ä¢ Or use specific scrapers for targeted data collection
‚Ä¢ Monitor progress in the output areas

STEP 3: Load Data into ChromaDB
‚Ä¢ Go to Vector Database ‚Üí Load Embeddings
‚Ä¢ Wait for processing to complete
‚Ä¢ Verify data was loaded successfully

STEP 4: Generate Hypotheses
‚Ä¢ Go to Hypothesis Generation ‚Üí Generate Hypotheses
‚Ä¢ Or start with Test Run for a quick demonstration
‚Ä¢ Review generated hypotheses and insights

STEP 5: Analyze Results
‚Ä¢ Use Settings ‚Üí Data Status to review your data
‚Ä¢ Check Vector Database ‚Üí Show ChromaDB Data for statistics
‚Ä¢ Export results for further analysis

üí° TIP: The system remembers your data between sessions - ChromaDB uses persistent storage."""
        
        workflow_label = ttk.Label(workflow_frame, text=workflow_text, justify=tk.LEFT, wraplength=800)
        workflow_label.pack(anchor=tk.W)
        
        # Configuration Tips
        config_frame = ttk.LabelFrame(parent, text="Configuration Tips ‚öôÔ∏è", padding=15)
        config_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        config_text = """Optimize your research with these configuration tips:

üîç Search Keywords
‚Ä¢ Customize keywords for your specific research focus
‚Ä¢ Use synonyms and related terms (e.g., protein names, disease terms, pathways)
‚Ä¢ Save keyword configurations for reuse

üìä PubMed Limits
‚Ä¢ Start with 1000-5000 papers for initial exploration
‚Ä¢ Use unlimited (-1) for comprehensive analysis (warning: takes hours)
‚Ä¢ Consider your computational resources

üóÑÔ∏è ChromaDB Management
‚Ä¢ Clear data when switching research topics
‚Ä¢ Monitor storage usage for large datasets
‚Ä¢ Use batch processing for efficiency

üß† Hypothesis Generation
‚Ä¢ Start with test queries to understand the system
‚Ä¢ Use specific, focused queries for better results
‚Ä¢ Review and refine generated hypotheses

üí° TIP: Check Settings ‚Üí Configurations to see current system settings."""
        
        config_label = ttk.Label(config_frame, text=config_text, justify=tk.LEFT, wraplength=800)
        config_label.pack(anchor=tk.W)
        
        # Troubleshooting
        troubleshooting_frame = ttk.LabelFrame(parent, text="Troubleshooting üîß", padding=15)
        troubleshooting_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        troubleshooting_text = """Common issues and solutions:

‚ùå "No ChromaDB collections found"
‚Ä¢ Solution: Run "Load Embeddings" first to populate the database

‚ùå "No preprint dumps found"
‚Ä¢ Solution: The system will automatically download dumps when needed
‚Ä¢ Ensure you have internet connection

‚ùå "Keywords not saved"
‚Ä¢ Solution: Check file permissions in the project directory
‚Ä¢ Try running as administrator if needed

‚ùå "Import errors"
‚Ä¢ Solution: Install missing dependencies: pip install pandas paperscraper chromadb
‚Ä¢ Check Python version compatibility

‚ùå "Long processing times"
‚Ä¢ Solution: This is normal for large datasets
‚Ä¢ Use smaller PubMed limits for faster initial runs
‚Ä¢ Monitor progress in output areas

üí° TIP: Check the output areas in each tab for detailed error messages and progress updates."""
        
        troubleshooting_label = ttk.Label(troubleshooting_frame, text=troubleshooting_text, justify=tk.LEFT, wraplength=800)
        troubleshooting_label.pack(anchor=tk.W)
        
        # Advanced Features
        advanced_frame = ttk.LabelFrame(parent, text="Advanced Features üöÄ", padding=15)
        advanced_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        advanced_text = """Explore these advanced features for power users:

üî¨ Custom Research Focus
‚Ä¢ Modify keywords for specific protein families or disease areas
‚Ä¢ Target specific disease types or biological pathways
‚Ä¢ Focus on particular research methodologies

üìà Data Analysis
‚Ä¢ Export data for external analysis tools
‚Ä¢ Use citation networks for research mapping
‚Ä¢ Analyze temporal trends in research

ü§ñ AI Customization
‚Ä¢ Adjust RAG parameters for different hypothesis types
‚Ä¢ Fine-tune search strategies
‚Ä¢ Experiment with different query formulations

üîç Research Validation
‚Ä¢ Cross-reference hypotheses with recent literature
‚Ä¢ Validate findings against multiple data sources
‚Ä¢ Track hypothesis evolution over time

üí° TIP: The system is designed to be extensible - check the documentation for advanced customization options."""
        
        advanced_label = ttk.Label(advanced_frame, text=advanced_text, justify=tk.LEFT, wraplength=800)
        advanced_label.pack(anchor=tk.W)
        
        # Getting Help
        help_frame = ttk.LabelFrame(parent, text="Getting Help üìû", padding=15)
        help_frame.pack(fill=tk.X, padx=20, pady=(0, 20))
        
        help_text = """Need assistance? Here's how to get help:

üìñ Documentation
‚Ä¢ Check GUI_README.md for detailed GUI instructions
‚Ä¢ Review README.md for system overview
‚Ä¢ Consult processing_config.py for configuration options

üîç Status Monitoring
‚Ä¢ Use Settings ‚Üí Data Status for system overview
‚Ä¢ Check Vector Database ‚Üí Show ChromaDB Data for database status
‚Ä¢ Monitor output areas for real-time progress

üêõ Error Reporting
‚Ä¢ Check output areas for detailed error messages
‚Ä¢ Look for error codes and suggested solutions
‚Ä¢ Note the specific operation that failed

üí° Best Practices
‚Ä¢ Start with small datasets to learn the system
‚Ä¢ Save your keyword configurations
‚Ä¢ Regularly check data status
‚Ä¢ Use test runs before large-scale operations

üí° TIP: The system provides helpful error messages and suggestions in the output areas."""
        
        help_label = ttk.Label(help_frame, text=help_text, justify=tk.LEFT, wraplength=800)
        help_label.pack(anchor=tk.W)
        
        # Footer
        footer_label = ttk.Label(parent, text="Happy Researching! üß¨", 
                               font=('Arial', 14, 'bold'))
        footer_label.pack(pady=30)
        
    # GUI Methods for Paper Scraping & Processing
    
    def load_saved_keywords(self):
        """Load saved keywords from configuration file."""
        try:
            if os.path.exists("config/search_keywords_config.json"):
                with open("config/search_keywords_config.json", 'r') as f:
                    config = json.load(f)
                self.pubmed_keywords_var.set(config.get("pubmed_keywords", ""))
                self.semantic_keywords_var.set(config.get("semantic_keywords", ""))
        except Exception as e:
            print(f"Could not load keywords config: {e}")
            
    def load_saved_keywords_unified(self):
        """Load saved keywords from configuration file for unified format."""
        try:
            if os.path.exists("config/search_keywords_config.json"):
                with open("config/search_keywords_config.json", 'r') as f:
                    config = json.load(f)
                # Use pubmed_keywords as the unified keywords (they should be the same)
                unified_keywords = config.get("pubmed_keywords", "")
                self.full_scraper_keywords_var.set(unified_keywords)
        except Exception as e:
            print(f"Could not load keywords config: {e}")
            
    def save_keywords_gui(self):
        """Save keywords configuration."""
        try:
            config = {
                "pubmed_keywords": self.pubmed_keywords_var.get(),
                "semantic_keywords": self.semantic_keywords_var.get(),
                "last_updated": json.dumps({"timestamp": __import__("datetime").datetime.now().isoformat()})
            }
            
            with open("config/search_keywords_config.json", 'w') as f:
                json.dump(config, f, indent=2)
            
            messagebox.showinfo("Success", "Keywords saved successfully!")
        except Exception as e:
            messagebox.showerror("Error", f"Could not save keywords: {e}")
            
    def save_keywords_unified_gui(self):
        """Save unified keywords configuration."""
        try:
            unified_keywords = self.full_scraper_keywords_var.get()
            config = {
                "pubmed_keywords": unified_keywords,
                "semantic_keywords": unified_keywords,
                "last_updated": json.dumps({"timestamp": __import__("datetime").datetime.now().isoformat()})
            }
            
            with open("config/search_keywords_config.json", 'w') as f:
                json.dump(config, f, indent=2)
            
            messagebox.showinfo("Success", "Keywords saved successfully!")
        except Exception as e:
            messagebox.showerror("Error", f"Could not save keywords: {e}")
            
    def run_full_scraper_gui(self):
        """Run full scraper in a separate thread."""
        if not self.full_scraper_keywords_var.get().strip():
            messagebox.showerror("Error", "Please enter keywords")
            return
            
        try:
            max_results = int(self.max_results_var.get())
        except ValueError:
            messagebox.showerror("Error", "Please enter a valid number for max results")
            return
            
        # Run in separate thread
        thread = threading.Thread(target=self._run_full_scraper_thread, 
                                args=(max_results,))
        thread.daemon = True
        thread.start()
        
    def _run_full_scraper_thread(self, max_results):
        """Thread function for full scraper."""
        self.start_progress()
        self.update_status("Running full scraper...")
        
        # Create progress wrapper
        progress_wrapper = ScraperProgressWrapper(
            self, 
            self.full_scraper_progress, 
            self.full_scraper_output
        )
        
        try:
            self.log_message(self.full_scraper_output, "Starting Full Scraper (Preprints + Journal Articles)")
            self.log_message(self.full_scraper_output, "="*60)
            
            success_count = 0
            total_sources = 3
            
            # Step 1: Process PubMed
            self.log_message(self.full_scraper_output, "Step 1/3: Processing PubMed (Journal Articles)...")
            try:
                self.log_message(self.full_scraper_output, f"Using keywords: {self.full_scraper_keywords_var.get()}")
                progress_wrapper.run_pubmed_with_progress(max_results=max_results)
                self.log_message(self.full_scraper_output, "‚úÖ PubMed processing completed successfully!")
                success_count += 1
            except Exception as e:
                self.log_message(self.full_scraper_output, f"‚ùå PubMed processing failed: {e}")
            
            # Step 2: Process xrvix
            self.log_message(self.full_scraper_output, "Step 2/3: Processing xrvix (Preprints)...")
            try:
                progress_wrapper.run_xrvix_with_progress()
                self.log_message(self.full_scraper_output, "‚úÖ xrvix processing completed successfully!")
                success_count += 1
            except Exception as e:
                self.log_message(self.full_scraper_output, f"‚ùå xrvix processing failed: {e}")
            
            # Step 3: Process UBR5
            self.log_message(self.full_scraper_output, "Step 3/3: Processing UBR5 (Semantic Scholar)...")
            try:
                self.log_message(self.full_scraper_output, f"Using keywords: {self.full_scraper_keywords_var.get()}")
                progress_wrapper.run_ubr5_with_progress(self.full_scraper_keywords_var.get())
                self.log_message(self.full_scraper_output, "‚úÖ UBR5 processing completed successfully!")
                success_count += 1
            except Exception as e:
                self.log_message(self.full_scraper_output, f"‚ùå UBR5 processing failed: {e}")
            
            # Summary
            self.log_message(self.full_scraper_output, f"üéâ Full scraper completed!")
            self.log_message(self.full_scraper_output, f"‚úÖ Successfully processed: {success_count}/{total_sources} sources")
            
        except Exception as e:
            self.log_message(self.full_scraper_output, f"‚ùå Unexpected error: {e}")
        finally:
            self.stop_progress()
            self.update_status("Ready")
            
    def run_journal_articles_gui(self):
        """Run journal articles scraper in a separate thread."""
        if not self.journal_keywords_var.get().strip():
            messagebox.showerror("Error", "Please enter keywords")
            return
            
        try:
            max_results = int(self.journal_max_results_var.get())
        except ValueError:
            messagebox.showerror("Error", "Please enter a valid number for max results")
            return
            
        # Run in separate thread
        thread = threading.Thread(target=self._run_journal_articles_thread, 
                                args=(max_results,))
        thread.daemon = True
        thread.start()
        
    def _run_journal_articles_thread(self, max_results):
        """Thread function for journal articles scraper."""
        self.start_progress()
        self.update_status("Running journal articles scraper...")
        
        # Create progress wrapper
        progress_wrapper = ScraperProgressWrapper(
            self, 
            self.journal_progress, 
            self.journal_output
        )
        
        try:
            self.log_message(self.journal_output, "Starting Journal Articles Scraper (PubMed + Semantic Scholar)")
            self.log_message(self.journal_output, "="*60)
            
            success_count = 0
            total_sources = 2
            
            # Step 1: Process PubMed
            self.log_message(self.journal_output, "Step 1/2: Processing PubMed...")
            try:
                self.log_message(self.journal_output, f"Using keywords: {self.journal_keywords_var.get()}")
                progress_wrapper.run_pubmed_with_progress(max_results=max_results)
                self.log_message(self.journal_output, "‚úÖ PubMed processing completed successfully!")
                success_count += 1
            except Exception as e:
                self.log_message(self.journal_output, f"‚ùå PubMed processing failed: {e}")
            
            # Step 2: Process UBR5
            self.log_message(self.journal_output, "Step 2/2: Processing UBR5 (Semantic Scholar)...")
            try:
                self.log_message(self.journal_output, f"Using keywords: {self.journal_keywords_var.get()}")
                progress_wrapper.run_ubr5_with_progress(self.journal_keywords_var.get())
                self.log_message(self.journal_output, "‚úÖ UBR5 processing completed successfully!")
                success_count += 1
            except Exception as e:
                self.log_message(self.journal_output, f"‚ùå UBR5 processing failed: {e}")
            
            # Summary
            self.log_message(self.journal_output, f"üéâ Journal articles scraper completed!")
            self.log_message(self.journal_output, f"‚úÖ Successfully processed: {success_count}/{total_sources} sources")
            
        except Exception as e:
            self.log_message(self.journal_output, f"‚ùå Unexpected error: {e}")
        finally:
            self.stop_progress()
            self.update_status("Ready")
            
    def check_preprints_status(self):
        """Check preprint data status."""
        try:
            dump_dir = "data/scraped_data/paperscraper_dumps"
            if os.path.exists(dump_dir):
                biorxiv_dumps = [f for f in os.listdir(dump_dir) if f.startswith('biorxiv')]
                medrxiv_dumps = [f for f in os.listdir(dump_dir) if f.startswith('medrxiv')]
                
                if biorxiv_dumps or medrxiv_dumps:
                    status_text = f"‚úÖ Found {len(biorxiv_dumps)} Biorxiv dumps and {len(medrxiv_dumps)} Medrxiv dumps"
                else:
                    status_text = "‚ùå No preprint dumps found - will download automatically"
            else:
                status_text = "‚ùå No preprint dumps directory found - will download automatically"
                
            self.preprints_status_label.config(text=status_text)
        except Exception as e:
            self.preprints_status_label.config(text=f"‚ùå Error checking status: {e}")
            
    def run_preprints_gui(self):
        """Run preprints scraper in a separate thread."""
        # Run in separate thread
        thread = threading.Thread(target=self._run_preprints_thread)
        thread.daemon = True
        thread.start()
        
    def _run_preprints_thread(self):
        """Thread function for preprints scraper."""
        self.start_progress()
        self.update_status("Running preprints scraper...")
        
        # Create progress wrapper
        progress_wrapper = ScraperProgressWrapper(
            self, 
            self.preprints_progress, 
            self.preprints_output
        )
        
        try:
            self.log_message(self.preprints_output, "Starting Preprints Scraper (Biorxiv, Medrxiv)")
            self.log_message(self.preprints_output, "="*60)
            
            progress_wrapper.run_xrvix_with_progress()
            self.log_message(self.preprints_output, "‚úÖ Preprints processing completed successfully!")
            
        except Exception as e:
            self.log_message(self.preprints_output, f"‚ùå Preprints processing failed: {e}")
        finally:
            self.stop_progress()
            self.update_status("Ready")
            
    def check_embeddings_status(self):
        """Check embeddings status."""
        try:
            if os.path.exists("data/embeddings/xrvix_embeddings"):
                status_text = "‚úÖ Embeddings directory exists - embeddings are generated during scraping"
            else:
                status_text = "‚ùå No embeddings found - run scraping options first"
                
            self.embeddings_status_label.config(text=status_text)
        except Exception as e:
            self.embeddings_status_label.config(text=f"‚ùå Error checking status: {e}")
            
    def generate_embeddings_gui(self):
        """Generate embeddings."""
        self.log_message(self.embeddings_output, "Embedding generation is typically done during the scraping process.")
        self.log_message(self.embeddings_output, "If you need to regenerate embeddings, run the scraping options first.")
        
        if os.path.exists("data/embeddings/xrvix_embeddings"):
            self.log_message(self.embeddings_output, "‚úÖ Embeddings directory exists")
        else:
            self.log_message(self.embeddings_output, "‚ùå No embeddings found - run scraping options first")
            
    # GUI Methods for Vector Database Management
    
    def check_chromadb_status(self):
        """Check ChromaDB status."""
        try:
            manager = ChromaDBManager()
            collections = manager.list_collections()
            
            if collections:
                stats = manager.get_collection_stats()
                total_docs = stats.get('total_documents', 0)
                status_text = f"‚úÖ ChromaDB available with {total_docs} documents in {len(collections)} collection(s)"
            else:
                status_text = "‚ö†Ô∏è ChromaDB available but no collections found"
                
            self.load_status_label.config(text=status_text)
        except Exception as e:
            self.load_status_label.config(text=f"‚ùå Error checking ChromaDB: {e}")
            
    def load_embeddings_gui(self):
        """Load embeddings into ChromaDB in a separate thread."""
        thread = threading.Thread(target=self._load_embeddings_thread)
        thread.daemon = True
        thread.start()
        
    def _load_embeddings_thread(self):
        """Thread function for loading embeddings."""
        self.start_progress()
        self.update_status("Loading embeddings into ChromaDB...")
        
        try:
            self.log_message(self.load_output, "Loading embeddings into ChromaDB...")
            self.log_message(self.load_output, "="*60)
            
            # Initialize ChromaDB manager
            manager = ChromaDBManager()
            
            # Create collection
            if not manager.create_collection():
                self.log_message(self.load_output, "‚ùå Failed to create ChromaDB collection")
                return
            
            # Check if collection already has data
            stats = manager.get_collection_stats()
            if stats.get('total_documents', 0) > 0:
                self.log_message(self.load_output, f"üìö ChromaDB already has {stats.get('total_documents', 0)} documents!")
                self.log_message(self.load_output, "üí° ChromaDB uses persistent storage - data is already saved locally.")
                
                # Ask user if they want to reload anyway
                result = messagebox.askyesno("Reload Data?", 
                                           "ChromaDB already has data. Do you want to reload anyway?")
                if not result:
                    self.log_message(self.load_output, "‚úÖ Using existing ChromaDB data.")
                    return
            
            total_loaded = 0
            
            # Load PubMed embeddings
            pubmed_path = "data/embeddings/xrvix_embeddings/pubmed_embeddings.json"
            if os.path.exists(pubmed_path):
                self.log_message(self.load_output, "üîÑ Loading PubMed embeddings...")
                pubmed_data = manager.load_embeddings_from_json(pubmed_path)
                if pubmed_data:
                    if manager.add_embeddings_to_collection(pubmed_data, "pubmed"):
                        total_loaded += len(pubmed_data.get('embeddings', []))
                        self.log_message(self.load_output, f"‚úÖ Loaded {len(pubmed_data.get('embeddings', []))} PubMed embeddings")
            
            # Load all embeddings from xrvix directory (auto-detects all sources)
            xrvix_path = "data/embeddings/xrvix_embeddings"
            if os.path.exists(xrvix_path):
                self.log_message(self.load_output, "üîÑ Loading all embeddings from xrvix directory (auto-detecting sources)...")
                if manager.add_embeddings_from_directory(xrvix_path, db_batch_size=DB_BATCH_SIZE):
                    stats = manager.get_collection_stats()
                    total_docs = stats.get('total_documents', 0)
                    self.log_message(self.load_output, f"‚úÖ Loaded all embeddings (total documents: {total_docs})")
                    total_loaded = total_docs
            
            # Load Semantic Scholar API embeddings
            semantic_path = "data/embeddings/xrvix_embeddings/semantic_scholar"
            if os.path.exists(semantic_path):
                self.log_message(self.load_output, "üîÑ Loading Semantic Scholar API embeddings...")
                if manager.add_embeddings_from_directory(semantic_path, db_batch_size=DB_BATCH_SIZE):
                    stats = manager.get_collection_stats()
                    total_docs = stats.get('total_documents', 0)
                    self.log_message(self.load_output, f"‚úÖ Loaded Semantic Scholar API embeddings (total documents: {total_docs})")
                    total_loaded = total_docs
            
            if total_loaded == 0:
                self.log_message(self.load_output, "‚ö†Ô∏è No data was loaded. Make sure you have processed some data first.")
                return
            
            # Display final statistics
            stats = manager.get_collection_stats()
            self.log_message(self.load_output, f"\nüìä Vector Database Statistics:")
            self.log_message(self.load_output, f"   Total documents: {stats.get('total_documents', 0)}")
            self.log_message(self.load_output, f"   Collection name: {stats.get('collection_name', 'N/A')}")
            
            self.log_message(self.load_output, f"\n‚úÖ Successfully loaded {total_loaded} embeddings into vector database!")
            
        except Exception as e:
            self.log_message(self.load_output, f"‚ùå Error loading data into vector database: {e}")
        finally:
            self.stop_progress()
            self.update_status("Ready")
            
    def show_chromadb_data_gui(self):
        """Show ChromaDB data."""
        thread = threading.Thread(target=self._show_chromadb_data_thread)
        thread.daemon = True
        thread.start()
        
    def _show_chromadb_data_thread(self):
        """Thread function for showing ChromaDB data."""
        try:
            self.log_message(self.show_output, "Current ChromaDB Data:")
            self.log_message(self.show_output, "="*60)
            
            manager = ChromaDBManager()
            collections = manager.list_collections()
            
            if not collections:
                self.log_message(self.show_output, "‚ùå No collections found in ChromaDB")
                self.log_message(self.show_output, "üí° Run Load Embeddings to load data into ChromaDB")
                return
            
            self.log_message(self.show_output, f"‚úÖ ChromaDB is available with {len(collections)} collection(s)")
            
            # Check each collection
            for collection_name in collections:
                self.log_message(self.show_output, f"\nüìö Collection: {collection_name}")
                
                if not manager.switch_collection(collection_name):
                    self.log_message(self.show_output, "   ‚ùå Failed to access collection")
                    continue
                
                # Get statistics
                stats = manager.get_collection_stats()
                total_docs = stats.get('total_documents', 0)
                self.log_message(self.show_output, f"   üìä Total documents: {total_docs}")
                
                if total_docs == 0:
                    self.log_message(self.show_output, "   ‚ö†Ô∏è Collection is empty")
                else:
                    self.log_message(self.show_output, "   ‚úÖ Collection has data - ready for searching!")
                    
                    # Show source breakdown
                    batch_stats = manager.get_batch_statistics()
                    if batch_stats and batch_stats.get('sources'):
                        self.log_message(self.show_output, "   üì¶ Source breakdown:")
                        for source, source_stats in batch_stats['sources'].items():
                            self.log_message(self.show_output, f"      {source}: {source_stats['total_documents']} documents")
            
            self.log_message(self.show_output, f"\nüí° ChromaDB uses persistent storage - data is saved locally")
            
        except Exception as e:
            self.log_message(self.show_output, f"‚ùå Error checking ChromaDB status: {e}")
            
    def refresh_chromadb_data(self):
        """Refresh ChromaDB data display."""
        self.show_output.delete(1.0, tk.END)
        self.show_chromadb_data_gui()
        
    def clear_chromadb_data_gui(self):
        """Clear ChromaDB data."""
        if not self.confirm_var.get():
            messagebox.showerror("Confirmation Required", "Please check the confirmation box first")
            return
            
        result = messagebox.askyesno("Confirm Deletion", 
                                   "Are you sure you want to delete all ChromaDB data?\nThis action cannot be undone!")
        if not result:
            return
            
        thread = threading.Thread(target=self._clear_chromadb_data_thread)
        thread.daemon = True
        thread.start()
        
    def _clear_chromadb_data_thread(self):
        """Thread function for clearing ChromaDB data."""
        self.start_progress()
        self.update_status("Clearing ChromaDB data...")
        
        try:
            self.log_message(self.clear_output, "Clearing ChromaDB Data:")
            self.log_message(self.clear_output, "="*60)
            
            manager = ChromaDBManager()
            
            # Initialize collection first to ensure it's accessible
            if manager.create_collection():
                if manager.clear_collection():
                    self.log_message(self.clear_output, "‚úÖ ChromaDB collection cleared successfully!")
                else:
                    self.log_message(self.clear_output, "‚ùå Failed to clear ChromaDB collection")
            else:
                self.log_message(self.clear_output, "‚ùå Failed to initialize ChromaDB collection")
                
        except Exception as e:
            self.log_message(self.clear_output, f"‚ùå Error clearing ChromaDB: {e}")
        finally:
            self.stop_progress()
            self.update_status("Ready")
            
    # GUI Methods for Settings & Config
    
    def show_data_status_gui(self):
        """Show data status."""
        thread = threading.Thread(target=self._show_data_status_thread)
        thread.daemon = True
        thread.start()
        
    def _show_data_status_thread(self):
        """Thread function for showing data status."""
        try:
            self.log_message(self.data_status_output, "Data Status Overview:")
            self.log_message(self.data_status_output, "=" * 50)
            
            # Check PubMed data
            self.log_message(self.data_status_output, "\nüìö PubMed Data (Journal Articles):")
            pubmed_path = "data/embeddings/xrvix_embeddings/pubmed_embeddings.json"
            if os.path.exists(pubmed_path):
                try:
                    with open(pubmed_path, 'r') as f:
                        pubmed_data = json.load(f)
                        pubmed_count = len(pubmed_data.get('embeddings', []))
                        size = os.path.getsize(pubmed_path) / 1024
                        self.log_message(self.data_status_output, f"   ‚úÖ Available: {pubmed_count} papers ({size:.1f} KB)")
                except Exception as e:
                    self.log_message(self.data_status_output, f"   ‚ö†Ô∏è Available but error reading: {e}")
            else:
                self.log_message(self.data_status_output, "   ‚ùå Not available")
            
            # Check Biorxiv data
            self.log_message(self.data_status_output, "\nüìÑ Biorxiv Data (Preprints):")
            biorxiv_path = "data/embeddings/xrvix_embeddings/biorxiv"
            if os.path.exists(biorxiv_path):
                try:
                    batch_files = [f for f in os.listdir(biorxiv_path) if f.startswith("batch_") and f.endswith(".json")]
                    self.log_message(self.data_status_output, f"   ‚úÖ Available: {len(batch_files)} batch files")
                    if batch_files:
                        # Try to get total count from first batch
                        first_batch = os.path.join(biorxiv_path, batch_files[0])
                        with open(first_batch, 'r') as f:
                            batch_data = json.load(f)
                            sample_count = len(batch_data.get('embeddings', []))
                            estimated_total = sample_count * len(batch_files)
                            self.log_message(self.data_status_output, f"   üìä Estimated papers: ~{estimated_total}")
                except Exception as e:
                    self.log_message(self.data_status_output, f"   ‚ö†Ô∏è Available but error reading: {e}")
            else:
                self.log_message(self.data_status_output, "   ‚ùå Not available")
            
            # Check Medrxiv data
            self.log_message(self.data_status_output, "\nüìÑ Medrxiv Data (Preprints):")
            medrxiv_path = "data/embeddings/xrvix_embeddings/medrxiv"
            if os.path.exists(medrxiv_path):
                try:
                    batch_files = [f for f in os.listdir(medrxiv_path) if f.startswith("batch_") and f.endswith(".json")]
                    self.log_message(self.data_status_output, f"   ‚úÖ Available: {len(batch_files)} batch files")
                    if batch_files:
                        # Try to get total count from first batch
                        first_batch = os.path.join(medrxiv_path, batch_files[0])
                        with open(first_batch, 'r') as f:
                            batch_data = json.load(f)
                            sample_count = len(batch_data.get('embeddings', []))
                            estimated_total = sample_count * len(batch_files)
                            self.log_message(self.data_status_output, f"   üìä Estimated papers: ~{estimated_total}")
                except Exception as e:
                    self.log_message(self.data_status_output, f"   ‚ö†Ô∏è Available but error reading: {e}")
            else:
                self.log_message(self.data_status_output, "   ‚ùå Not available")
            
            # Check Semantic Scholar API data
            self.log_message(self.data_status_output, "\nüî¨ Semantic Scholar API Data:")
            semantic_path = "data/embeddings/xrvix_embeddings/semantic_scholar"
            if os.path.exists(semantic_path):
                try:
                    paper_files = [f for f in os.listdir(semantic_path) if f.endswith('.json') and f != 'metadata.json']
                    metadata_file = os.path.join(semantic_path, "metadata.json")
                    if os.path.exists(metadata_file):
                        with open(metadata_file, 'r') as f:
                            semantic_data = json.load(f)
                            semantic_count = semantic_data.get('total_papers', 0)
                            self.log_message(self.data_status_output, f"   ‚úÖ Available: {semantic_count} papers")
                    else:
                        self.log_message(self.data_status_output, f"   ‚úÖ Available: {len(paper_files)} individual paper files")
                except Exception as e:
                    self.log_message(self.data_status_output, f"   ‚ö†Ô∏è Available but error reading: {e}")
            else:
                self.log_message(self.data_status_output, "   ‚ùå Not available")
            
            # Check ChromaDB
            self.log_message(self.data_status_output, "\nüóÑÔ∏è ChromaDB Vector Database:")
            if os.path.exists("data/vector_db/chroma_db"):
                try:
                    manager = ChromaDBManager()
                    collections = manager.list_collections()
                    if collections:
                        stats = manager.get_collection_stats()
                        total_docs = stats.get('total_documents', 0)
                        self.log_message(self.data_status_output, f"   ‚úÖ Available: {total_docs} documents")
                    else:
                        self.log_message(self.data_status_output, "   ‚ö†Ô∏è Available but empty")
                except Exception as e:
                    self.log_message(self.data_status_output, f"   ‚ö†Ô∏è Available but error accessing: {e}")
            else:
                self.log_message(self.data_status_output, "   ‚ùå Not available")
            
            self.log_message(self.data_status_output, "\nüí° Recommendations:")
            self.log_message(self.data_status_output, "   ‚Ä¢ Use Paper Scraping options to collect missing data sources")
            self.log_message(self.data_status_output, "   ‚Ä¢ Use Load Embeddings to load data into ChromaDB")
            self.log_message(self.data_status_output, "   ‚Ä¢ Use Generate Hypotheses to generate hypotheses")
            
        except Exception as e:
            self.log_message(self.data_status_output, f"‚ùå Error checking data status: {e}")
            
    def refresh_data_status(self):
        """Refresh data status display."""
        self.data_status_output.delete(1.0, tk.END)
        self.show_data_status_gui()
        
    def show_configurations_gui(self):
        """Show configurations."""
        try:
            self.log_message(self.config_output, "Current Configurations:")
            self.log_message(self.config_output, "="*60)
            
            # Capture print_config_info output
            import io
            import sys
            from contextlib import redirect_stdout
            
            f = io.StringIO()
            with redirect_stdout(f):
                print_config_info()
            config_info = f.getvalue()
            
            self.log_message(self.config_output, config_info)
            
        except Exception as e:
            self.log_message(self.config_output, f"‚ùå Error showing configurations: {e}")
            
    # GUI Methods for Hypothesis Generation
    
    def check_hypothesis_prerequisites(self):
        """Check prerequisites for hypothesis generation."""
        try:
            manager = ChromaDBManager()
            collections = manager.list_collections()
            
            if not collections:
                status_text = "‚ùå No ChromaDB collections found - run Load Embeddings first"
            else:
                # Ensure collection is properly initialized
                if not manager.create_collection():
                    status_text = "‚ùå Failed to initialize ChromaDB collection"
                else:
                    stats = manager.get_collection_stats()
                    if stats.get('total_documents', 0) == 0:
                        status_text = "‚ùå ChromaDB is empty - run Load Embeddings first"
                    else:
                        status_text = f"‚úÖ ChromaDB has {stats.get('total_documents', 0)} documents - ready for hypothesis generation"
                    
            self.prereq_status_label.config(text=status_text)
        except Exception as e:
            self.prereq_status_label.config(text=f"‚ùå Error checking prerequisites: {e}")
            
    def generate_hypotheses_gui(self):
        """Generate hypotheses using the GUI input fields."""
        try:
            # Get input values
            prompt = self.hypothesis_prompt.get("1.0", tk.END).strip()
            lab_name = self.hypothesis_lab.get().strip()
            institution = self.hypothesis_institution.get().strip()
            hypotheses_per_meta_str = self.hypotheses_per_meta.get().strip()
            
            # Check if prompt is placeholder text
            placeholder_text = "Enter your research question or hypothesis prompt here...\nExample: How does UBR5 regulate cancer immunity through protein ubiquitination?"
            if prompt == placeholder_text:
                prompt = ""
            
            # Validate inputs
            if not prompt:
                self.log_message(self.hypothesis_output, "‚ùå Please enter a research prompt")
                return
            
            if not lab_name:
                self.log_message(self.hypothesis_output, "‚ùå Please enter a lab name")
                return
                
            if not institution:
                self.log_message(self.hypothesis_output, "‚ùå Please enter an institution")
                return
            
            # Validate hypotheses per meta-hypothesis
            try:
                hypotheses_per_meta = int(hypotheses_per_meta_str)
                if hypotheses_per_meta < 1 or hypotheses_per_meta > 10:
                    self.log_message(self.hypothesis_output, "‚ùå Hypotheses per meta-hypothesis must be between 1 and 10")
                    return
            except ValueError:
                self.log_message(self.hypothesis_output, "‚ùå Please enter a valid number for hypotheses per meta-hypothesis")
                return
            
            # Clear output and start progress
            self.hypothesis_output.delete("1.0", tk.END)
            self.log_progress("Starting hypothesis generation...", 0, start_animation=True)
            
            # Log input parameters
            self.log_message(self.hypothesis_output, "üß† Generating Hypotheses with Parameters:")
            self.log_message(self.hypothesis_output, f"üìù Prompt: {prompt}")
            self.log_message(self.hypothesis_output, f"üî¨ Lab: {lab_name}")
            self.log_message(self.hypothesis_output, f"üèõÔ∏è Institution: {institution}")
            self.log_message(self.hypothesis_output, f"üìä Hypotheses per Meta-Hypothesis: {hypotheses_per_meta}")
            self.log_message(self.hypothesis_output, "=" * 60)
            
            # Check ChromaDB prerequisites
            self.log_progress("Checking ChromaDB prerequisites...", 10)
            manager = ChromaDBManager()
            collections = manager.list_collections()
            
            if not collections:
                self.log_message(self.hypothesis_output, "‚ùå No ChromaDB collections found")
                self.log_message(self.hypothesis_output, "üí° Run Load Embeddings to load data into ChromaDB first")
                self.update_progress("Failed - No ChromaDB collections", stop_animation=True)
                return
            
            # Ensure collection is properly initialized
            self.log_progress("Initializing ChromaDB collection...", 20)
            if not manager.create_collection():
                self.log_message(self.hypothesis_output, "‚ùå Failed to initialize ChromaDB collection")
                self.update_progress("Failed - ChromaDB initialization error", stop_animation=True)
                return
            
            stats = manager.get_collection_stats()
            if stats.get('total_documents', 0) == 0:
                self.log_message(self.hypothesis_output, "‚ùå ChromaDB is empty")
                self.log_message(self.hypothesis_output, "üí° Run Load Embeddings to load data into ChromaDB first")
                self.update_progress("Failed - ChromaDB is empty", stop_animation=True)
                return
            
            self.log_message(self.hypothesis_output, f"‚úÖ ChromaDB has {stats.get('total_documents', 0)} documents")
            self.log_progress("Initializing Enhanced RAG System...", 30)
            
            # Import and initialize the enhanced RAG system
            from src.ai.enhanced_rag_with_chromadb import EnhancedRAGQuery
            
            # Initialize the RAG system
            rag_system = EnhancedRAGQuery(use_chromadb=True, load_data_at_startup=True)
            
            self.log_message(self.hypothesis_output, "‚úÖ RAG System initialized successfully")
            self.log_progress("Starting meta-hypothesis generation...", 40)
            
            # Generate hypotheses using the RAG system
            self.generate_hypotheses_with_rag(rag_system, prompt, lab_name, institution, hypotheses_per_meta)
            
        except Exception as e:
            self.log_message(self.hypothesis_output, f"‚ùå Failed to generate hypotheses: {e}")
            import traceback
            self.log_message(self.hypothesis_output, f"üìã Error details: {traceback.format_exc()}")
            self.update_progress("Failed - Error occurred", stop_animation=True)
    
    def generate_hypotheses_with_rag(self, rag_system, prompt, lab_name, institution, hypotheses_per_meta=3):
        """Generate hypotheses using the RAG system with custom parameters."""
        try:
            # Update lab configuration temporarily
            import json
            lab_config = {
                "lab_name": lab_name,
                "institution": institution,
                "research_focus": "UBR5, cancer immunology, protein ubiquitination, mechanistic and therapeutic hypotheses"
            }
            
            # Save temporary config
            with open("config/temp_lab_config.json", "w") as f:
                json.dump(lab_config, f)
            
            self.log_message(self.hypothesis_output, f"üìã Using lab configuration: {lab_name} at {institution}")
            
            # Generate hypotheses using the comprehensive session
            self.log_progress("Generating meta-hypotheses (Step 1/3)...", 50)
            self.log_message(self.hypothesis_output, "üß† Generating hypotheses with comprehensive evaluation...")
            
            # Use the comprehensive hypothesis generation method
            self.log_progress("Processing meta-hypotheses (Step 2/3)...", 70)
            
            # Create progress callback for the RAG system
            def progress_callback(step_text, percentage=None):
                self.log_progress(step_text, percentage)
            
            accepted_hypotheses = rag_system.run_comprehensive_hypothesis_session(
                prompt, 
                max_hypotheses=3, 
                hypotheses_per_meta=hypotheses_per_meta,
                progress_callback=progress_callback
            )
            
            if accepted_hypotheses:
                self.log_progress("Formatting results (Step 3/3)...", 90)
                self.log_message(self.hypothesis_output, "‚úÖ Hypotheses generated successfully!")
                self.log_message(self.hypothesis_output, "=" * 60)
                
                for i, hypothesis_record in enumerate(accepted_hypotheses, 1):
                    self.log_message(self.hypothesis_output, f"üìã HYPOTHESIS {i}:")
                    
                    # Extract hypothesis text from the record
                    if isinstance(hypothesis_record, dict):
                        hypothesis_text = hypothesis_record.get('hypothesis', str(hypothesis_record))
                    else:
                        hypothesis_text = str(hypothesis_record)
                    
                    self.log_message(self.hypothesis_output, hypothesis_text)
                    self.log_message(self.hypothesis_output, "=" * 60)
                
                # Complete
                self.log_progress("‚úÖ Hypothesis generation completed successfully!", 100, stop_animation=True)
            else:
                self.log_message(self.hypothesis_output, "‚ùå No hypotheses were generated")
                self.update_progress("Failed - No hypotheses generated", stop_animation=True)
            
            # Clean up temporary config
            if os.path.exists("config/temp_lab_config.json"):
                os.remove("config/temp_lab_config.json")
                
        except Exception as e:
            self.log_message(self.hypothesis_output, f"‚ùå Error during hypothesis generation: {e}")
            import traceback
            self.log_message(self.hypothesis_output, f"üìã Error details: {traceback.format_exc()}")
            self.update_progress("Failed - Error during generation", stop_animation=True)
            
    def check_test_prerequisites(self):
        """Check prerequisites for test run."""
        try:
            manager = ChromaDBManager()
            collections = manager.list_collections()
            
            if not collections:
                status_text = "‚ùå No ChromaDB collections found - run Load Embeddings first"
            else:
                # Ensure collection is properly initialized
                if not manager.create_collection():
                    status_text = "‚ùå Failed to initialize ChromaDB collection"
                else:
                    stats = manager.get_collection_stats()
                    if stats.get('total_documents', 0) == 0:
                        status_text = "‚ùå ChromaDB is empty - run Load Embeddings first"
                    else:
                        status_text = f"‚úÖ ChromaDB has {stats.get('total_documents', 0)} documents - ready for test run"
                    
            self.test_prereq_status_label.config(text=status_text)
        except Exception as e:
            self.test_prereq_status_label.config(text=f"‚ùå Error checking prerequisites: {e}")
            
    def test_run_gui(self):
        """Run test."""
        try:
            manager = ChromaDBManager()
            collections = manager.list_collections()
            
            if not collections:
                self.log_message(self.test_output, "‚ùå No ChromaDB collections found")
                self.log_message(self.test_output, "üí° Run Load Embeddings to load data into ChromaDB first")
                return
            
            # Ensure collection is properly initialized
            if not manager.create_collection():
                self.log_message(self.test_output, "‚ùå Failed to initialize ChromaDB collection")
                return
            
            stats = manager.get_collection_stats()
            if stats.get('total_documents', 0) == 0:
                self.log_message(self.test_output, "‚ùå ChromaDB is empty")
                self.log_message(self.test_output, "üí° Run Load Embeddings to load data into ChromaDB first")
                return
            
            test_query = self.test_query_var.get()
            self.log_message(self.test_output, f"‚úÖ ChromaDB has {stats.get('total_documents', 0)} documents")
            self.log_message(self.test_output, f"üß™ Running test query: '{test_query}'")
            
            # Launch the enhanced RAG system with a test query
            self.log_message(self.test_output, "üöÄ Launching Enhanced RAG System for test run...")
            subprocess.run([sys.executable, "enhanced_rag_with_chromadb.py"])
            
        except Exception as e:
            self.log_message(self.test_output, f"‚ùå Failed to run test: {e}")


def main():
    """Main function to run the GUI application."""
    root = tk.Tk()
    app = AIResearchProcessorGUI(root)
    
    # Center the window
    root.update_idletasks()
    x = (root.winfo_screenwidth() // 2) - (root.winfo_width() // 2)
    y = (root.winfo_screenheight() // 2) - (root.winfo_height() // 2)
    root.geometry(f"+{x}+{y}")
    
    root.mainloop()


if __name__ == "__main__":
    main()
