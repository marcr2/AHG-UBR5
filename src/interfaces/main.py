#!/usr/bin/env python3
"""
AI Research Processor - Main Menu
AI-Powered Scientific Hypothesis Generator for Biomedical Research
"""

import os
import sys
import json
import subprocess
import logging
import traceback
import io
from contextlib import redirect_stdout, redirect_stderr

# Add the current directory to Python path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.scrapers.pubmed_scraper_json import main as process_pubmed
from src.scrapers.process_xrvix_dumps_json import main as process_xrvix
from src.scrapers.ubr5_api_scraper import UBR5APIScraper
from src.core.chromadb_manager import ChromaDBManager
from src.core.processing_config import print_config_info, get_config, DB_BATCH_SIZE

# Configure comprehensive logging
def setup_debug_logging():
    """Setup comprehensive debug logging for terminal interface."""
    # Create logs directory if it doesn't exist
    os.makedirs("data/logs", exist_ok=True)
    
    # Configure root logger
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('data/logs/terminal_debug.log', mode='a'),
            logging.StreamHandler(sys.stdout)  # Also output to console
        ]
    )
    
    # Set specific loggers to DEBUG level
    loggers_to_debug = [
        'pubmed_scraper_json',
        'process_xrvix_dumps_json', 
        'ubr5_api_scraper',
        'chromadb_manager',
        'xrvix_downloader',
        'paperscraper',
        'requests',
        'urllib3'
    ]
    
    for logger_name in loggers_to_debug:
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.DEBUG)
    
    return logging.getLogger(__name__)

# Initialize logging
logger = setup_debug_logging()

def show_menu():
    """Display the main menu with the specified structure."""
    print("\n" + "="*60)
    print("üöÄ AI RESEARCH PROCESSOR - MAIN MENU")
    print("="*60)
    
    print("\nüìö PAPER SCRAPING & PROCESSING:")
    print("1. Full scraper (preprints + journal articles) - Custom keywords")
    print("2. Journal articles only (pubmed, semantic scholar) - Custom keywords")
    print("3. Preprints only (Biorxiv, Medrxiv) - Automatically downloads and processes")
    print("4. Generate embeddings")
    
    print("\nüóÑÔ∏è VECTOR DATABASE MANAGEMENT:")
    print("5. Load embeddings")
    print("6. Show current ChromaDB data")
    print("7. Clear ChromaDB data")
    
    print("\n‚öôÔ∏è SETTINGS & CONFIG:")
    print("8. Show data status")
    print("9. Show configurations")
    
    print("\nüß† HYPOTHESIS GENERATION:")
    print("10. Generate hypotheses")
    print("11. Test run (Biomedical Research)")
    
    print("\nüîß DEBUG & LOGGING:")
    print("12. Enable/Disable Full Debug Logging")
    print("13. View Debug Log File")
    print("14. Clear Debug Logs")
    
    print("\n15. Exit")
    print("="*60)

def get_user_choice():
    """Get user choice with proper validation."""
    while True:
        try:
            choice = int(input("\nEnter your choice (1-15): "))
            if 1 <= choice <= 15:
                return choice
            else:
                print("‚ùå Please enter a number between 1 and 15.")
        except ValueError:
            print("‚ùå Please enter a valid number.")
        except KeyboardInterrupt:
            print("\nüëã Goodbye!")
            sys.exit(0)

# Global debug logging state
DEBUG_LOGGING_ENABLED = True

def toggle_debug_logging():
    """Toggle full debug logging on/off."""
    global DEBUG_LOGGING_ENABLED
    
    print("\nüîß Debug Logging Configuration")
    print("="*50)
    print(f"Current status: {'ENABLED' if DEBUG_LOGGING_ENABLED else 'DISABLED'}")
    print()
    print("Full debug logging includes:")
    print("‚Ä¢ Detailed API request/response logs")
    print("‚Ä¢ Raw error messages and stack traces")
    print("‚Ä¢ Network connection details")
    print("‚Ä¢ Database operation logs")
    print("‚Ä¢ All internal processing steps")
    print()
    
    toggle = input("Toggle debug logging? (y/n): ").strip().lower()
    if toggle == 'y':
        DEBUG_LOGGING_ENABLED = not DEBUG_LOGGING_ENABLED
        print(f"‚úÖ Debug logging {'ENABLED' if DEBUG_LOGGING_ENABLED else 'DISABLED'}")
        
        # Update logging level
        if DEBUG_LOGGING_ENABLED:
            logging.getLogger().setLevel(logging.DEBUG)
            print("üîç All debug information will be displayed in terminal")
        else:
            logging.getLogger().setLevel(logging.INFO)
            print("‚ÑπÔ∏è  Only essential information will be displayed")
    else:
        print("‚ùå Debug logging configuration unchanged")

def view_debug_log():
    """View the debug log file."""
    log_file = "data/logs/terminal_debug.log"
    
    print("\nüìã Debug Log Viewer")
    print("="*50)
    
    if not os.path.exists(log_file):
        print("‚ùå No debug log file found")
        print("üí° Run some operations first to generate logs")
        return
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        if not lines:
            print("üìÑ Debug log file is empty")
            return
        
        print(f"üìÑ Debug log file: {log_file}")
        print(f"üìä Total lines: {len(lines)}")
        print()
        
        # Show last 50 lines by default
        show_lines = input("How many lines to show? (default: 50, 'all' for full log): ").strip()
        
        if show_lines.lower() == 'all':
            lines_to_show = lines
            print("üìÑ Showing full debug log:")
        else:
            try:
                num_lines = int(show_lines) if show_lines else 50
                lines_to_show = lines[-num_lines:] if num_lines < len(lines) else lines
                print(f"üìÑ Showing last {len(lines_to_show)} lines:")
            except ValueError:
                lines_to_show = lines[-50:]
                print("üìÑ Showing last 50 lines:")
        
        print("-" * 80)
        for line in lines_to_show:
            print(line.rstrip())
        print("-" * 80)
        
    except Exception as e:
        print(f"‚ùå Error reading debug log: {e}")
        logger.error(f"Error reading debug log: {e}", exc_info=True)

def clear_debug_logs():
    """Clear all debug logs."""
    print("\nüóëÔ∏è Clear Debug Logs")
    print("="*50)
    
    log_files = [
        "data/logs/terminal_debug.log",
        "data/logs/paper_processing.log", 
        "data/logs/ubr5_api_scraping.log"
    ]
    
    existing_files = [f for f in log_files if os.path.exists(f)]
    
    if not existing_files:
        print("üìÑ No log files found to clear")
        return
    
    print("Found log files:")
    for i, file in enumerate(existing_files, 1):
        size = os.path.getsize(file) / 1024  # Size in KB
        print(f"  {i}. {file} ({size:.1f} KB)")
    
    print()
    confirm = input("Clear all log files? (y/n): ").strip().lower()
    if confirm == 'y':
        cleared_count = 0
        for file in existing_files:
            try:
                with open(file, 'w') as f:
                    f.write("")  # Clear the file
                cleared_count += 1
                print(f"‚úÖ Cleared: {file}")
            except Exception as e:
                print(f"‚ùå Error clearing {file}: {e}")
        
        print(f"\nüéØ Cleared {cleared_count}/{len(existing_files)} log files")
    else:
        print("‚ùå Log clearing cancelled")

def get_search_keywords():
    """Get custom search keywords from user."""
    print("\nüîç Search Keywords Configuration")
    print("="*50)
    print("You can customize the search keywords for PubMed and Semantic Scholar.")
    print()
    
    # Check for previously saved keywords
    saved_pubmed, saved_semantic = load_keywords_config()
    
    if saved_pubmed and saved_semantic:
        print("üìÅ Previously saved keywords found:")
        print(f"   PubMed: {saved_pubmed}")
        print(f"   Semantic Scholar: {saved_semantic}")
        print()
        
        use_saved = input("Use previously saved keywords? (y/n): ").strip().lower()
        if use_saved == 'y':
            return saved_pubmed, saved_semantic
    
    print("Choose keyword configuration:")
    print("1. Use default keywords (biomedical research)")
    print("2. Enter custom keywords")
    print("3. Cancel")
    
    while True:
        choice = input("Enter choice (1-3): ").strip()
        if choice == '1':
            pubmed_keywords = "protein, disease, biomedical, research, biology"
            semantic_keywords = "protein, disease, biomedical, research, biology"
            print(f"   Using default PubMed: {pubmed_keywords}")
            print(f"   Using default Semantic Scholar: {semantic_keywords}")
            break
        elif choice == '2':
            # Get unified keywords for both PubMed and Semantic Scholar
            print("\nüîç Search Keywords:")
            print("   Default: protein, disease, biomedical, research, biology")
            print("   Note: Same keywords will be used for both PubMed and Semantic Scholar")
            unified_keywords = input("   Enter keywords (comma-separated, or press Enter for default): ").strip()
            
            if not unified_keywords:
                unified_keywords = "protein, disease, biomedical, research, biology"
                print(f"   Using default: {unified_keywords}")
            else:
                print(f"   Using custom: {unified_keywords}")
            
            # Use the same keywords for both sources
            pubmed_keywords = unified_keywords
            semantic_keywords = unified_keywords
            break
        elif choice == '3':
            print("‚ùå Search cancelled.")
            return None, None
        else:
            print("‚ùå Invalid choice. Please enter 1, 2, or 3.")
    
    print()
    
    # Confirm keywords
    print("üìã Search Configuration Summary:")
    print(f"   Keywords (used for both PubMed and Semantic Scholar): {pubmed_keywords}")
    print()
    
    confirm = input("Proceed with these keywords? (y/n): ").strip().lower()
    if confirm != 'y':
        print("‚ùå Search cancelled.")
        return None, None
    
    # Save keywords to configuration file for future use
    save_keywords_config(pubmed_keywords, semantic_keywords)
    
    return pubmed_keywords, semantic_keywords

def save_keywords_config(pubmed_keywords, semantic_keywords):
    """Save keywords configuration to file."""
    config = {
        "pubmed_keywords": pubmed_keywords,
        "semantic_keywords": semantic_keywords,
        "last_updated": json.dumps({"timestamp": __import__("datetime").datetime.now().isoformat()})
    }
    
    try:
        with open("config/search_keywords_config.json", 'w') as f:
            json.dump(config, f, indent=2)
        print("üíæ Keywords saved to search_keywords_config.json")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not save keywords config: {e}")

def load_keywords_config():
    """Load keywords configuration from file."""
    try:
        if os.path.exists("config/search_keywords_config.json"):
            with open("config/search_keywords_config.json", 'r') as f:
                config = json.load(f)
            return config.get("pubmed_keywords"), config.get("semantic_keywords")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not load keywords config: {e}")
    
    return None, None

def get_pubmed_max_results():
    """Get maximum number of PubMed results from user."""
    print("   Specify the maximum number of PubMed papers to retrieve:")
    print("   ‚Ä¢ Enter a positive number (e.g., 1000, 5000) for a specific limit")
    print("   ‚Ä¢ Enter -1 for unlimited results (use with caution)")
    print("   ‚Ä¢ Press Enter for default (5000)")
    
    while True:
        try:
            user_input = input("   Maximum PubMed results (default: 5000): ").strip()
            
            # If user presses Enter, use default
            if not user_input:
                max_results = 5000
                print(f"   ‚úÖ Using default: {max_results} papers")
                break
            
            # Parse user input
            max_results = int(user_input)
            
            if max_results == -1:
                print("   ‚ö†Ô∏è  Unlimited results selected - this may take a very long time!")
                confirm = input("   Are you sure? (y/n): ").strip().lower()
                if confirm == 'y':
                    max_results = 50000  # Set a very high limit instead of truly unlimited
                    print(f"   ‚úÖ Unlimited results mode: {max_results} papers maximum")
                    break
                else:
                    print("   ‚ùå Unlimited results cancelled, please enter a number")
                    continue
            
            elif max_results <= 0:
                print("   ‚ùå Please enter a positive number or -1 for unlimited")
                continue
            
            elif max_results > 50000:
                print(f"   ‚ö†Ô∏è  {max_results} is a very large number - this may take hours!")
                print(f"   Estimated time: {max_results // 1000} - {max_results // 500} minutes")
                confirm = input("   Are you sure? (y/n): ").strip().lower()
                if confirm == 'y':
                    print(f"   ‚úÖ Maximum results set to: {max_results}")
                    break
                else:
                    print("   ‚ùå Please enter a smaller number")
                    continue
            
            else:
                print(f"   ‚úÖ Maximum results set to: {max_results}")
                break
                
        except ValueError:
            print("   ‚ùå Please enter a valid number")
            continue
        except KeyboardInterrupt:
            print("\nüëã Search cancelled by user")
            return None
    
    return max_results

def run_full_scraper():
    """Run full scraper for preprints + journal articles."""
    print("\nüöÄ Starting Full Scraper (Preprints + Journal Articles)")
    print("="*60)
    
    # Get custom keywords from user
    pubmed_keywords, semantic_keywords = get_search_keywords()
    if pubmed_keywords is None or semantic_keywords is None:
        return
    
    success_count = 0
    total_sources = 3  # PubMed + xrvix + UBR5
    
    # Step 1: Process PubMed (journal articles)
    print("\nüìö Step 1/3: Processing PubMed (Journal Articles)...")
    try:
        # Ask user for max results for PubMed
        print(f"   Using keywords: {pubmed_keywords}")
        print("   üî¢ PubMed Results Configuration:")
        max_results = get_pubmed_max_results()
        if max_results is None:
            print("‚ùå PubMed processing cancelled by user")
            return
        
        process_pubmed(max_results=max_results)
        print("‚úÖ PubMed processing completed successfully!")
        success_count += 1
    except Exception as e:
        print(f"‚ùå PubMed processing failed: {e}")
    
    # Step 2: Process xrvix (preprints)
    print("\nüìÑ Step 2/3: Processing xrvix (Preprints)...")
    try:
        # Check if dump files exist and download if needed
        dump_dir = "data/scraped_data/paperscraper_dumps"
        server_dumps_dir = os.path.join(dump_dir, "server_dumps")
        
        # Check both old and new directory structures
        biorxiv_dumps = []
        medrxiv_dumps = []
        
        if os.path.exists(server_dumps_dir):
            biorxiv_dumps = [f for f in os.listdir(server_dumps_dir) if 'biorxiv' in f.lower()]
            medrxiv_dumps = [f for f in os.listdir(server_dumps_dir) if 'medrxiv' in f.lower()]
        elif os.path.exists(dump_dir):
            biorxiv_dumps = [f for f in os.listdir(dump_dir) if f.startswith('biorxiv')]
            medrxiv_dumps = [f for f in os.listdir(dump_dir) if f.startswith('medrxiv')]
        
        if not biorxiv_dumps and not medrxiv_dumps:
            print("   ‚ùå No preprint dumps found!")
            print("   üí° Downloading latest preprint dumps automatically...")
            
            # Automatically download dumps
            from src.scrapers.xrvix_downloader import XRXivDownloader
            downloader = XRXivDownloader()
            success = downloader.download_all_dumps()
            
            if not success:
                print("   ‚ùå Failed to download preprint dumps!")
                print("   üí° Skipping preprint processing")
                raise Exception("Failed to download preprint dumps")
            
            print("   ‚úÖ Preprint dumps downloaded successfully!")
        
        # xrvix processing doesn't use custom keywords (it processes existing dumps)
        process_xrvix()
        print("‚úÖ xrvix processing completed successfully!")
        success_count += 1
    except Exception as e:
        print(f"‚ùå xrvix processing failed: {e}")
    
    # Step 3: Process UBR5 (Semantic Scholar)
    print("\nüî¨ Step 3/3: Processing UBR5 (Semantic Scholar)...")
    try:
        ubr5_scraper = UBR5APIScraper()
        # Set custom keywords for UBR5 scraper
        ubr5_scraper.search_keywords = [keyword.strip() for keyword in semantic_keywords.split(',')]
        print(f"   Using keywords: {semantic_keywords}")
        ubr5_scraper.run_complete_scraping()
        print("‚úÖ UBR5 processing completed successfully!")
        success_count += 1
    except Exception as e:
        print(f"‚ùå UBR5 processing failed: {e}")
    
    # Summary
    print(f"\nüéâ Full scraper completed!")
    print(f"‚úÖ Successfully processed: {success_count}/{total_sources} sources")
    
    if success_count == total_sources:
        print("üéØ All sources processed successfully!")
    elif success_count > 0:
        print("‚ö†Ô∏è  Some sources failed, but others succeeded.")
    else:
        print("‚ùå All sources failed. Check error messages above.")

def run_journal_articles_only():
    """Run scraper for journal articles only (PubMed + Semantic Scholar)."""
    print("\nüìö Starting Journal Articles Scraper (PubMed + Semantic Scholar)")
    print("="*60)
    
    # Get custom keywords from user
    pubmed_keywords, semantic_keywords = get_search_keywords()
    if pubmed_keywords is None or semantic_keywords is None:
        return
    
    success_count = 0
    total_sources = 2  # PubMed + UBR5
    
    # Step 1: Process PubMed
    print("\nüìö Step 1/2: Processing PubMed...")
    try:
        # Ask user for max results for PubMed
        print(f"   Using keywords: {pubmed_keywords}")
        print("   üî¢ PubMed Results Configuration:")
        max_results = get_pubmed_max_results()
        if max_results is None:
            print("‚ùå PubMed processing cancelled by user")
            return
        
        process_pubmed(max_results=max_results)
        print("‚úÖ PubMed processing completed successfully!")
        success_count += 1
    except Exception as e:
        print(f"‚ùå PubMed processing failed: {e}")
    
    # Step 2: Process UBR5 (Semantic Scholar)
    print("\nüî¨ Step 2/2: Processing UBR5 (Semantic Scholar)...")
    try:
        ubr5_scraper = UBR5APIScraper()
        # Set custom keywords for UBR5 scraper
        ubr5_scraper.search_keywords = [keyword.strip() for keyword in semantic_keywords.split(',')]
        print(f"   Using keywords: {semantic_keywords}")
        ubr5_scraper.run_complete_scraping()
        print("‚úÖ UBR5 processing completed successfully!")
        success_count += 1
    except Exception as e:
        print(f"‚ùå UBR5 processing failed: {e}")
    
    # Summary
    print(f"\nüéâ Journal articles scraper completed!")
    print(f"‚úÖ Successfully processed: {success_count}/{total_sources} sources")

def run_preprints_only():
    """Run scraper for preprints only (Biorxiv, Medrxiv)."""
    print("\nüìÑ Starting Preprints Scraper (Biorxiv, Medrxiv)")
    print("="*60)
    
    print("‚ÑπÔ∏è  Note: Preprint processing uses existing dump files.")
    print("   Custom keywords are not applicable for this option.")
    print("   The system will process all available preprint data.")
    print()
    
    try:
        # Check if dump files exist
        dump_dir = "data/scraped_data/paperscraper_dumps"
        server_dumps_dir = os.path.join(dump_dir, "server_dumps")
        
        # Check both old and new directory structures
        biorxiv_dumps = []
        medrxiv_dumps = []
        
        if os.path.exists(server_dumps_dir):
            biorxiv_dumps = [f for f in os.listdir(server_dumps_dir) if 'biorxiv' in f.lower()]
            medrxiv_dumps = [f for f in os.listdir(server_dumps_dir) if 'medrxiv' in f.lower()]
        elif os.path.exists(dump_dir):
            biorxiv_dumps = [f for f in os.listdir(dump_dir) if f.startswith('biorxiv')]
            medrxiv_dumps = [f for f in os.listdir(dump_dir) if f.startswith('medrxiv')]
        
        if not biorxiv_dumps and not medrxiv_dumps:
            print("‚ùå No preprint dumps found!")
            print("üí° Downloading latest preprint dumps automatically...")
            print()
            
            # Automatically download dumps
            from src.scrapers.xrvix_downloader import XRXivDownloader
            downloader = XRXivDownloader()
            success = downloader.download_all_dumps()
            
            if not success:
                print("‚ùå Failed to download preprint dumps!")
                print("üí° Please check your internet connection and try again")
                return
            
            print("‚úÖ Preprint dumps downloaded successfully!")
            print("üîÑ Now processing the downloaded dumps...")
            print()
        else:
            print(f"‚úÖ Found {len(biorxiv_dumps)} Biorxiv dumps and {len(medrxiv_dumps)} Medrxiv dumps")
            print("üîÑ Processing existing dumps...")
            print()
        
        confirm = input("Proceed with preprint processing? (y/n): ").strip().lower()
        if confirm != 'y':
            print("‚ùå Preprint processing cancelled.")
            return
        
        process_xrvix()
        print("‚úÖ Preprints processing completed successfully!")
    except Exception as e:
        print(f"‚ùå Preprints processing failed: {e}")
        print("üí° Make sure you have an internet connection for downloading dumps")

def generate_embeddings():
    """Generate embeddings for processed data."""
    print("\nüîÑ Starting Embedding Generation...")
    print("="*60)
    
    try:
        # This would typically involve running the embedding generation process
        # For now, we'll check if data exists and provide guidance
        if os.path.exists("data/embeddings/xrvix_embeddings"):
            print("‚úÖ Embeddings directory exists")
            print("üí° Embeddings are generated during the scraping process")
            print("   If you need to regenerate embeddings, run the scraping options first")
        else:
            print("‚ùå No embeddings found")
            print("üí° Run scraping options (1-3) first to generate embeddings")
    except Exception as e:
        print(f"‚ùå Error checking embeddings: {e}")

def load_embeddings():
    """Load embeddings into ChromaDB."""
    print("\nüóÑÔ∏è Loading embeddings into ChromaDB...")
    print("="*60)
    
    try:
        # Initialize ChromaDB manager
        manager = ChromaDBManager()
        
        # Create collection
        if not manager.create_collection():
            print("‚ùå Failed to create ChromaDB collection")
            return False
        
        # Check if collection already has data
        stats = manager.get_collection_stats()
        if stats.get('total_documents', 0) > 0:
            print(f"üìö ChromaDB already has {stats.get('total_documents', 0)} documents!")
            print("üí° ChromaDB uses persistent storage - data is already saved locally.")
            
            # Ask user if they want to reload anyway
            reload_choice = input("\nDo you want to reload the data anyway? (y/n): ").strip().lower()
            if reload_choice != 'y':
                print("‚úÖ Using existing ChromaDB data.")
                return True
        
        total_loaded = 0
        
        # Load PubMed embeddings
        pubmed_path = "data/embeddings/xrvix_embeddings/pubmed_embeddings.json"
        if os.path.exists(pubmed_path):
            print("üîÑ Loading PubMed embeddings...")
            pubmed_data = manager.load_embeddings_from_json(pubmed_path)
            if pubmed_data:
                if manager.add_embeddings_to_collection(pubmed_data, "pubmed"):
                    total_loaded += len(pubmed_data.get('embeddings', []))
                    print(f"‚úÖ Loaded {len(pubmed_data.get('embeddings', []))} PubMed embeddings")
        
        # Load xrvix embeddings (biorxiv, medrxiv)
        xrvix_path = "data/embeddings/xrvix_embeddings"
        if os.path.exists(xrvix_path):
            print("üîÑ Loading xrvix embeddings (biorxiv, medrxiv)...")
            if manager.add_embeddings_from_directory(xrvix_path, db_batch_size=DB_BATCH_SIZE):
                stats = manager.get_collection_stats()
                total_docs = stats.get('total_documents', 0)
                print(f"‚úÖ Loaded xrvix embeddings (total documents: {total_docs})")
                total_loaded = total_docs
        
        # Load UBR5 API embeddings
        ubr5_path = "data/embeddings/xrvix_embeddings/ubr5_api"
        if os.path.exists(ubr5_path):
            print("üîÑ Loading UBR5 API embeddings...")
            if manager.add_embeddings_from_directory(ubr5_path, db_batch_size=DB_BATCH_SIZE):
                stats = manager.get_collection_stats()
                total_docs = stats.get('total_documents', 0)
                print(f"‚úÖ Loaded UBR5 API embeddings (total documents: {total_docs})")
                total_loaded = total_docs
        
        if total_loaded == 0:
            print("‚ö†Ô∏è  No data was loaded. Make sure you have processed some data first.")
            return False
        
        # Display final statistics
        stats = manager.get_collection_stats()
        print(f"\nüìä Vector Database Statistics:")
        print(f"   Total documents: {stats.get('total_documents', 0)}")
        print(f"   Collection name: {stats.get('collection_name', 'N/A')}")
        
        print(f"\n‚úÖ Successfully loaded {total_loaded} embeddings into vector database!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error loading data into vector database: {e}")
        return False

def show_chromadb_data():
    """Show current ChromaDB data."""
    print("\nüóÑÔ∏è Current ChromaDB Data:")
    print("="*60)
    
    try:
        manager = ChromaDBManager()
        collections = manager.list_collections()
        
        if not collections:
            print("‚ùå No collections found in ChromaDB")
            print("üí° Run option 5 to load data into ChromaDB")
            return
        
        print(f"‚úÖ ChromaDB is available with {len(collections)} collection(s)")
        
        # Check each collection
        for collection_name in collections:
            print(f"\nüìö Collection: {collection_name}")
            
            if not manager.switch_collection(collection_name):
                print("   ‚ùå Failed to access collection")
                continue
            
            # Get statistics
            stats = manager.get_collection_stats()
            total_docs = stats.get('total_documents', 0)
            print(f"   üìä Total documents: {total_docs}")
            
            if total_docs == 0:
                print("   ‚ö†Ô∏è  Collection is empty")
            else:
                print("   ‚úÖ Collection has data - ready for searching!")
                
                # Show source breakdown
                batch_stats = manager.get_batch_statistics()
                if batch_stats and batch_stats.get('sources'):
                    print("   üì¶ Source breakdown:")
                    for source, source_stats in batch_stats['sources'].items():
                        print(f"      {source}: {source_stats['total_documents']} documents")
        
        print(f"\nüí° ChromaDB uses persistent storage - data is saved locally")
        
    except Exception as e:
        print(f"‚ùå Error checking ChromaDB status: {e}")

def clear_chromadb_data():
    """Clear ChromaDB data."""
    print("\nüóëÔ∏è Clearing ChromaDB Data:")
    print("="*60)
    
    try:
        manager = ChromaDBManager()
        
        # Initialize collection first to ensure it's accessible
        if manager.create_collection():
            if manager.clear_collection():
                print("‚úÖ ChromaDB collection cleared successfully!")
            else:
                print("‚ùå Failed to clear ChromaDB collection")
        else:
            print("‚ùå Failed to initialize ChromaDB collection")
            
    except Exception as e:
        print(f"‚ùå Error clearing ChromaDB: {e}")

def show_data_status():
    """Show the status of available data."""
    print("\nüìä Data Status Overview:")
    print("=" * 50)
    
    # Check PubMed data
    print("\nüìö PubMed Data (Journal Articles):")
    pubmed_path = "data/embeddings/xrvix_embeddings/pubmed_embeddings.json"
    if os.path.exists(pubmed_path):
        try:
            with open(pubmed_path, 'r') as f:
                pubmed_data = json.load(f)
                pubmed_count = len(pubmed_data.get('embeddings', []))
                size = os.path.getsize(pubmed_path) / 1024
                print(f"   ‚úÖ Available: {pubmed_count} papers ({size:.1f} KB)")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Available but error reading: {e}")
    else:
        print("   ‚ùå Not available")
    
    # Check Biorxiv data
    print("\nüìÑ Biorxiv Data (Preprints):")
    biorxiv_path = "data/embeddings/xrvix_embeddings/biorxiv"
    if os.path.exists(biorxiv_path):
        try:
            batch_files = [f for f in os.listdir(biorxiv_path) if f.startswith("batch_") and f.endswith(".json")]
            print(f"   ‚úÖ Available: {len(batch_files)} batch files")
            if batch_files:
                # Try to get total count from first batch
                first_batch = os.path.join(biorxiv_path, batch_files[0])
                with open(first_batch, 'r') as f:
                    batch_data = json.load(f)
                    sample_count = len(batch_data.get('embeddings', []))
                    estimated_total = sample_count * len(batch_files)
                    print(f"   üìä Estimated papers: ~{estimated_total}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Available but error reading: {e}")
    else:
        print("   ‚ùå Not available")
    
    # Check Medrxiv data
    print("\nüìÑ Medrxiv Data (Preprints):")
    medrxiv_path = "data/embeddings/xrvix_embeddings/medrxiv"
    if os.path.exists(medrxiv_path):
        try:
            batch_files = [f for f in os.listdir(medrxiv_path) if f.startswith("batch_") and f.endswith(".json")]
            print(f"   ‚úÖ Available: {len(batch_files)} batch files")
            if batch_files:
                # Try to get total count from first batch
                first_batch = os.path.join(medrxiv_path, batch_files[0])
                with open(first_batch, 'r') as f:
                    batch_data = json.load(f)
                    sample_count = len(batch_data.get('embeddings', []))
                    estimated_total = sample_count * len(batch_files)
                    print(f"   üìä Estimated papers: ~{estimated_total}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Available but error reading: {e}")
    else:
        print("   ‚ùå Not available")
    
    # Check UBR5 API data
    print("\nüî¨ UBR5 API Data (Semantic Scholar):")
    ubr5_path = "data/embeddings/xrvix_embeddings/ubr5_api"
    if os.path.exists(ubr5_path):
        try:
            paper_files = [f for f in os.listdir(ubr5_path) if f.endswith('.json') and f != 'metadata.json']
            metadata_file = os.path.join(ubr5_path, "metadata.json")
            if os.path.exists(metadata_file):
                with open(metadata_file, 'r') as f:
                    ubr5_data = json.load(f)
                    ubr5_count = ubr5_data.get('total_papers', 0)
                    print(f"   ‚úÖ Available: {ubr5_count} papers")
            else:
                print(f"   ‚úÖ Available: {len(paper_files)} individual paper files")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Available but error reading: {e}")
    else:
        print("   ‚ùå Not available")
    
    # Check ChromaDB
    print("\nüóÑÔ∏è ChromaDB Vector Database:")
    if os.path.exists("data/vector_db/chroma_db"):
        try:
            manager = ChromaDBManager()
            collections = manager.list_collections()
            if collections:
                stats = manager.get_collection_stats()
                total_docs = stats.get('total_documents', 0)
                print(f"   ‚úÖ Available: {total_docs} documents")
            else:
                print("   ‚ö†Ô∏è  Available but empty")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Available but error accessing: {e}")
    else:
        print("   ‚ùå Not available")
    
    print("\nüí° Recommendations:")
    print("   ‚Ä¢ Use options 1-3 to collect missing data sources")
    print("   ‚Ä¢ Use option 5 to load data into ChromaDB")
    print("   ‚Ä¢ Use option 10 to generate hypotheses")

def show_configurations():
    """Show current configurations."""
    print("\n‚öôÔ∏è Current Configurations:")
    print("="*60)
    print_config_info()

def generate_hypotheses():
    """Launch the hypothesis generation system."""
    print("\nüß† Starting Hypothesis Generation System...")
    print("="*60)
    
    try:
        # Check if ChromaDB has data
        manager = ChromaDBManager()
        collections = manager.list_collections()
        
        if not collections:
            print("‚ùå No ChromaDB collections found")
            print("üí° Run option 5 to load data into ChromaDB first")
            return
        
        # Ensure collection is properly initialized
        if not manager.create_collection():
            print("‚ùå Failed to initialize ChromaDB collection")
            return
        
        stats = manager.get_collection_stats()
        if stats.get('total_documents', 0) == 0:
            print("‚ùå ChromaDB is empty")
            print("üí° Run option 5 to load data into ChromaDB first")
            return
        
        print(f"‚úÖ ChromaDB has {stats.get('total_documents', 0)} documents")
        print("üöÄ Launching Enhanced RAG System...")
        
        # Launch the enhanced RAG system
        subprocess.run([sys.executable, "src/ai/enhanced_rag_with_chromadb.py"])
        
    except Exception as e:
        print(f"‚ùå Failed to launch hypothesis generation system: {e}")

def test_run_ubr5():
    """Run a test with UBR5 tumor immunology focus."""
    print("\nüß™ Test Run: UBR5 Tumor Immunology")
    print("="*60)
    
    try:
        # Check if ChromaDB has data
        manager = ChromaDBManager()
        collections = manager.list_collections()
        
        if not collections:
            print("‚ùå No ChromaDB collections found")
            print("üí° Run option 5 to load data into ChromaDB first")
            return
        
        # Ensure collection is properly initialized
        if not manager.create_collection():
            print("‚ùå Failed to initialize ChromaDB collection")
            return
        
        stats = manager.get_collection_stats()
        if stats.get('total_documents', 0) == 0:
            print("‚ùå ChromaDB is empty")
            print("üí° Run option 5 to load data into ChromaDB first")
            return
        
        print(f"‚úÖ ChromaDB has {stats.get('total_documents', 0)} documents")
        print("üß™ Running test query: 'protein function and disease'")
        
        # Launch the enhanced RAG system with a test query
        print("üöÄ Launching Enhanced RAG System for test run...")
        subprocess.run([sys.executable, "src/ai/enhanced_rag_with_chromadb.py"])
        
    except Exception as e:
        print(f"‚ùå Failed to run test: {e}")

def execute_with_debug_logging(func, *args, **kwargs):
    """Execute a function with comprehensive debug logging and error handling."""
    global DEBUG_LOGGING_ENABLED
    
    logger.info(f"Starting execution: {func.__name__}")
    logger.debug(f"Function: {func.__name__}, Args: {args}, Kwargs: {kwargs}")
    
    try:
        # Capture stdout and stderr for detailed logging
        stdout_capture = io.StringIO()
        stderr_capture = io.StringIO()
        
        with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
            result = func(*args, **kwargs)
        
        # Log captured output
        stdout_content = stdout_capture.getvalue()
        stderr_content = stderr_capture.getvalue()
        
        if stdout_content:
            logger.debug(f"STDOUT from {func.__name__}:\n{stdout_content}")
        if stderr_content:
            logger.debug(f"STDERR from {func.__name__}:\n{stderr_content}")
        
        logger.info(f"Successfully completed: {func.__name__}")
        return result
        
    except Exception as e:
        # Log the full error with stack trace
        logger.error(f"Error in {func.__name__}: {str(e)}", exc_info=True)
        
        # Display raw error information
        print(f"\nüîç RAW ERROR DETAILS:")
        print("="*60)
        print(f"Error Type: {type(e).__name__}")
        print(f"Error Message: {str(e)}")
        print(f"\nFull Stack Trace:")
        print("-" * 40)
        traceback.print_exc()
        print("-" * 40)
        
        # Also log to file
        logger.error(f"Full stack trace for {func.__name__}:", exc_info=True)
        
        return None

def main():
    """Main function with the new menu structure."""
    print("üöÄ Welcome to AI Research Processor!")
    print("üí° AI-Powered Scientific Hypothesis Generator for Biomedical Research")
    print("üìö This system helps generate novel research hypotheses from scientific literature")
    print(f"üîß Debug Logging: {'ENABLED' if DEBUG_LOGGING_ENABLED else 'DISABLED'}")
    
    while True:
        try:
            show_menu()
            choice = get_user_choice()
            
            logger.info(f"User selected option: {choice}")
            
            if choice == 1:
                execute_with_debug_logging(run_full_scraper)
                
            elif choice == 2:
                execute_with_debug_logging(run_journal_articles_only)
                
            elif choice == 3:
                execute_with_debug_logging(run_preprints_only)
                
            elif choice == 4:
                execute_with_debug_logging(generate_embeddings)
                
            elif choice == 5:
                execute_with_debug_logging(load_embeddings)
                
            elif choice == 6:
                execute_with_debug_logging(show_chromadb_data)
                
            elif choice == 7:
                execute_with_debug_logging(clear_chromadb_data)
                
            elif choice == 8:
                execute_with_debug_logging(show_data_status)
                
            elif choice == 9:
                execute_with_debug_logging(show_configurations)
                
            elif choice == 10:
                execute_with_debug_logging(generate_hypotheses)
                
            elif choice == 11:
                execute_with_debug_logging(test_run_ubr5)
                
            elif choice == 12:
                toggle_debug_logging()
                
            elif choice == 13:
                view_debug_log()
                
            elif choice == 14:
                clear_debug_logs()
                
            elif choice == 15:
                print("\nüëã Goodbye! Thank you for using AI Research Processor!")
                logger.info("User exited the application")
                break
                
            else:
                print("‚ùå Invalid choice. Please try again.")
                logger.warning(f"Invalid choice entered: {choice}")
            
            if choice not in [12, 13, 14, 15]:  # Don't pause for debug/logging options
                input("\nPress Enter to continue...")
                
        except KeyboardInterrupt:
            print("\n\nüëã Application interrupted by user")
            logger.info("Application interrupted by user (Ctrl+C)")
            break
        except Exception as e:
            print(f"\n‚ùå Unexpected error in main loop: {e}")
            logger.error(f"Unexpected error in main loop: {e}", exc_info=True)
            print("\nüîç RAW ERROR DETAILS:")
            print("="*60)
            traceback.print_exc()
            print("="*60)
            input("\nPress Enter to continue...")

if __name__ == "__main__":
    main()
